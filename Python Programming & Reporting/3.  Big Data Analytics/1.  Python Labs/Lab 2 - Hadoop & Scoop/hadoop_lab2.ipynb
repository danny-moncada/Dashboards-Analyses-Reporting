{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cloudera@quickstart data]$ mysql -uroot -pcloudera dualcore\n",
    "Reading table information for completion of table and column names\n",
    "You can turn off this feature to get a quicker startup with -A\n",
    "\n",
    "Welcome to the MySQL monitor.  Commands end with ; or \\g.\n",
    "Your MySQL connection id is 428\n",
    "Server version: 5.1.73 Source distribution\n",
    "\n",
    "Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.\n",
    "\n",
    "Oracle is a registered trademark of Oracle Corporation and/or its\n",
    "affiliates. Other names may be trademarks of their respective\n",
    "owners.\n",
    "\n",
    "Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n",
    "\n",
    "mysql>\n",
    "\n",
    "mysql> SHOW tables;\n",
    "+--------------------+\n",
    "| Tables_in_dualcore |\n",
    "+--------------------+\n",
    "| customers          |\n",
    "| employees          |\n",
    "| order_details      |\n",
    "| orders             |\n",
    "| products           |\n",
    "| suppliers          |\n",
    "+--------------------+\n",
    "6 rows in set (0.00 sec)\n",
    "\n",
    "mysql> DESCRIBE employees;\n",
    "+-----------+-------------+------+-----+---------+-------+\n",
    "| Field     | Type        | Null | Key | Default | Extra |\n",
    "+-----------+-------------+------+-----+---------+-------+\n",
    "| emp_id    | char(9)     | NO   | PRI | NULL    |       |\n",
    "| fname     | varchar(15) | YES  |     | NULL    |       |\n",
    "| lname     | varchar(20) | YES  |     | NULL    |       |\n",
    "| address   | varchar(40) | YES  |     | NULL    |       |\n",
    "| city      | varchar(30) | YES  |     | NULL    |       |\n",
    "| state     | char(2)     | YES  |     | NULL    |       |\n",
    "| zipcode   | char(5)     | YES  |     | NULL    |       |\n",
    "| job_title | varchar(35) | YES  |     | NULL    |       |\n",
    "| email     | varchar(25) | YES  |     | NULL    |       |\n",
    "| active    | char(1)     | NO   |     | Y       |       |\n",
    "| salary    | int(11)     | YES  |     | NULL    |       |\n",
    "+-----------+-------------+------+-----+---------+-------+\n",
    "11 rows in set (0.00 sec)\n",
    "\n",
    "mysql> SELECT emp_id, fname, lname, state, salary FROM employees LIMIT 10;\n",
    "+-----------+---------+-----------+-------+--------+\n",
    "| emp_id    | fname   | lname     | state | salary |\n",
    "+-----------+---------+-----------+-------+--------+\n",
    "| AA1130960 | Amy     | Alicea    | KY    |  22329 |\n",
    "| AA1146303 | Anna    | Atkins    | CA    |  99645 |\n",
    "| AA1154964 | Annie   | Albritton | WV    |  26717 |\n",
    "| AA1352280 | Antoine | Aguirre   | AL    |  26078 |\n",
    "| AA1411429 | Arthur  | Andersen  | MS    |  17486 |\n",
    "| AA1418885 | Amanda  | Atkinson  | CA    |  22278 |\n",
    "| AA1510726 | Ann     | Askew     | KY    |  17220 |\n",
    "| AA1567042 | Anne    | Almonte   | CA    |  18566 |\n",
    "| AA1609979 | Anthony | Allen     | PA    |  20575 |\n",
    "| AA1636177 | Anthony | Aguilar   | CO    |  25262 |\n",
    "+-----------+---------+-----------+-------+--------+\n",
    "10 rows in set (0.00 sec)\n",
    "\n",
    "\n",
    "[cloudera@quickstart data]$ sqoop import \\\n",
    "> --connect jdbc:mysql://localhost/dualcore \\\n",
    "> --username root --password cloudera \\\n",
    "> --warehouse-dir /dualcore \\\n",
    "> --table employees\n",
    "19/06/14 16:45:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0\n",
    "19/06/14 16:45:49 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
    "19/06/14 16:45:50 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
    "19/06/14 16:45:50 INFO tool.CodeGenTool: Beginning code generation\n",
    "19/06/14 16:45:52 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
    "19/06/14 16:45:52 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
    "19/06/14 16:45:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
    "19/06/14 16:45:52 ERROR orm.CompilationManager: It seems as though you are running sqoop with a JRE.\n",
    "19/06/14 16:45:52 ERROR orm.CompilationManager: Sqoop requires a JDK that can compile Java code.\n",
    "19/06/14 16:45:52 ERROR orm.CompilationManager: Please install a JDK and set $JAVA_HOME to use it.\n",
    "19/06/14 16:45:52 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Could not start Java compiler.\n",
    "        at org.apache.sqoop.orm.CompilationManager.compile(CompilationManager.java:187)\n",
    "        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:108)\n",
    "        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)\n",
    "        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)\n",
    "        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)\n",
    "        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
    "        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)\n",
    "        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)\n",
    "        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)\n",
    "        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)\n",
    "\n",
    "\n",
    "[cloudera@quickstart data]$ sqoop import --connect jdbc:mysql://localhost/dualcore --username root --password cloudera --warehouse-dir /dualcore --table employees\n",
    "19/06/14 17:01:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0\n",
    "19/06/14 17:01:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
    "19/06/14 17:01:49 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
    "19/06/14 17:01:49 INFO tool.CodeGenTool: Beginning code generation\n",
    "19/06/14 17:01:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
    "19/06/14 17:01:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
    "19/06/14 17:01:50 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
    "Note: /tmp/sqoop-cloudera/compile/affda1c3e695afa54a4a83b88cf27554/employees.java uses or overrides a deprecated API.\n",
    "Note: Recompile with -Xlint:deprecation for details.\n",
    "19/06/14 17:01:57 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/affda1c3e695afa54a4a83b88cf27554/employees.jar\n",
    "19/06/14 17:01:57 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
    "19/06/14 17:01:57 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
    "19/06/14 17:01:57 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
    "19/06/14 17:01:57 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
    "19/06/14 17:01:57 INFO mapreduce.ImportJobBase: Beginning import of employees\n",
    "19/06/14 17:01:57 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
    "19/06/14 17:01:59 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
    "19/06/14 17:02:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
    "19/06/14 17:02:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "19/06/14 17:02:05 WARN hdfs.DFSClient: Caught exception\n",
    "java.lang.InterruptedException\n",
    "        at java.lang.Object.wait(Native Method)\n",
    "        at java.lang.Thread.join(Thread.java:1252)\n",
    "        at java.lang.Thread.join(Thread.java:1326)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:951)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:689)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:878)\n",
    "19/06/14 17:02:05 WARN hdfs.DFSClient: Caught exception\n",
    "java.lang.InterruptedException\n",
    "        at java.lang.Object.wait(Native Method)\n",
    "        at java.lang.Thread.join(Thread.java:1252)\n",
    "        at java.lang.Thread.join(Thread.java:1326)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:951)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:689)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:878)\n",
    "19/06/14 17:02:05 WARN hdfs.DFSClient: Caught exception\n",
    "java.lang.InterruptedException\n",
    "        at java.lang.Object.wait(Native Method)\n",
    "        at java.lang.Thread.join(Thread.java:1252)\n",
    "        at java.lang.Thread.join(Thread.java:1326)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:951)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:689)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:878)\n",
    "19/06/14 17:02:07 WARN hdfs.DFSClient: Caught exception\n",
    "java.lang.InterruptedException\n",
    "        at java.lang.Object.wait(Native Method)\n",
    "        at java.lang.Thread.join(Thread.java:1252)\n",
    "        at java.lang.Thread.join(Thread.java:1326)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:951)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:689)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:878)\n",
    "19/06/14 17:02:08 INFO db.DBInputFormat: Using read commited transaction isolation\n",
    "19/06/14 17:02:08 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`emp_id`), MAX(`emp_id`) FROM `employees`\n",
    "19/06/14 17:02:08 WARN db.TextSplitter: Generating splits for a textual index column.\n",
    "19/06/14 17:02:08 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.\n",
    "19/06/14 17:02:08 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.\n",
    "19/06/14 17:02:08 WARN hdfs.DFSClient: Caught exception\n",
    "java.lang.InterruptedException\n",
    "        at java.lang.Object.wait(Native Method)\n",
    "        at java.lang.Thread.join(Thread.java:1252)\n",
    "        at java.lang.Thread.join(Thread.java:1326)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:951)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:689)\n",
    "        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:878)\n",
    "19/06/14 17:02:08 INFO mapreduce.JobSubmitter: number of splits:6\n",
    "19/06/14 17:02:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1560184236553_0001\n",
    "19/06/14 17:02:10 INFO impl.YarnClientImpl: Submitted application application_1560184236553_0001\n",
    "19/06/14 17:02:11 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1560184236553_0001/\n",
    "19/06/14 17:02:11 INFO mapreduce.Job: Running job: job_1560184236553_0001\n",
    "19/06/14 17:02:36 INFO mapreduce.Job: Job job_1560184236553_0001 running in uber mode : false\n",
    "19/06/14 17:02:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "19/06/14 17:03:37 INFO mapreduce.Job:  map 17% reduce 0%\n",
    "19/06/14 17:03:42 INFO mapreduce.Job:  map 33% reduce 0%\n",
    "19/06/14 17:03:44 INFO mapreduce.Job:  map 50% reduce 0%\n",
    "19/06/14 17:03:46 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "19/06/14 17:03:47 INFO mapreduce.Job: Job job_1560184236553_0001 completed successfully\n",
    "19/06/14 17:03:47 INFO mapreduce.Job: Counters: 31\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=0\n",
    "                FILE: Number of bytes written=884754\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=823\n",
    "                HDFS: Number of bytes written=6706056\n",
    "                HDFS: Number of read operations=24\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=12\n",
    "        Job Counters\n",
    "                Killed map tasks=1\n",
    "                Launched map tasks=6\n",
    "                Other local map tasks=6\n",
    "                Total time spent by all maps in occupied slots (ms)=382347\n",
    "                Total time spent by all reduces in occupied slots (ms)=0\n",
    "                Total time spent by all map tasks (ms)=382347\n",
    "                Total vcore-seconds taken by all map tasks=382347\n",
    "                Total megabyte-seconds taken by all map tasks=391523328\n",
    "        Map-Reduce Framework\n",
    "                Map input records=61712\n",
    "                Map output records=61712\n",
    "                Input split bytes=823\n",
    "                Spilled Records=0\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=0\n",
    "                GC time elapsed (ms)=8550\n",
    "                CPU time spent (ms)=26800\n",
    "                Physical memory (bytes) snapshot=1396301824\n",
    "                Virtual memory (bytes) snapshot=16803762176\n",
    "                Total committed heap usage (bytes)=1458044928\n",
    "        File Input Format Counters\n",
    "                Bytes Read=0\n",
    "        File Output Format Counters\n",
    "                Bytes Written=6706056\n",
    "19/06/14 17:03:47 INFO mapreduce.ImportJobBase: Transferred 6.3954 MB in 105.778 seconds (61.9116 KB/sec)\n",
    "19/06/14 17:03:47 INFO mapreduce.ImportJobBase: Retrieved 61712 records.\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
