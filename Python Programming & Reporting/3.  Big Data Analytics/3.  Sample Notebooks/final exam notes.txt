Cloud Computing & AWS
-------------------------------

Cloud computing: Model for ubiquitous, convenient, on-demand network access to shared pool of configurable computing resources (networks, servers, storage, apps) that can be rapidly
provisioned and released with minimal management effort.

* Create new apps/services
* Store/backup/recover data
* Host websites/blogs
* Stream audio/video
* Deliver software on demand
* Analyze data for patterns/predictions

Cloud Deployment Types

* Public: General public use based on shared physical hardware, owned/operated by third party like AWS, Azure, Google Cloud
* Private: Use of single entity, hosted on-site or service provider data center
* Hybrid: Combo of public and private cloud, public cloud for non-sensitive operations, private cloud for business critical ops

Cloud Computing Services

* Infrastructure as a Service (IaaS): Basic building blocks of computing: processing, network connectivity, storage.  VMs, storage, servers, network services.
*** EC2, Azure, Rackspace, Google Compute Engine
* Platform as a Service (PaaS): Platform for devs to build customized applications.  Speed up development.
*** Google App Engine, Azure Websites, Force.com
* Software as a Service (SaaS): Use web to deliver applications managed by third party vendor (bug fixes, upgrades, back-end data mgmt) used by clients.
*** Google Apps, Concur, WebEx, Office 365

Virtualization: make something that doesn't actually (physically) exist appear to exist
* Microprocess virtualization
* Virtual memory
* Network virtualization
* Server virtualization (VM)

Containerization

* Container: Software process whose access has been reduced to point it thinks it is only thing running.

* Containerization (operating-system level virtualization): allows existence of multiple isolated user-space instances
** Docker: platform for managing containers

Kubernetes: platform hosting Docker containers in clustered environment with multiple Docker hosts
** Container platform
** Microservices platform (component of system that is independently releasable and independent scalable from other parts of system)
** Portable cloud platform

Inevitable move to the cloud: over 50% of global enterprise in 2018 with rely on public cloud technologies to implement digital transformation and drive customer experience.

AWS Infrastructure
* Auto scaling and Elastic load balancing (scale up or down based on demand)
* Deploy systems in multiple regions (lower latency and better experience)
* Tools to run a wide range of applications
* REGIONS (physical location)
* AVAILABILITY ZONES (Azs): one or more discrete data centers, redudant power/networking/connectivity, housed in separate facilities

COMPUTATION

EC2 (Elastic Compute Cloud): Amazon virtual servers
* Obtain and boot new server instances in minutes
* Quickly scale capacity, up and down, as requirements change
* Reserved and spot instances

STORAGE

S3 (Simple Storage Service): Amazon's unlimited FTP server to store files/images/etc
* Scalable, secure, highly available, durable
* Static content, data for analytics, back ups

** Stores objects in buckets
*** Buckets can hold any number of objects, files up to 5TB.  Bucket name must be globally unique.
*** Flat directory structure

EBS (Elastic Block Store): Virtual hard disk used by EC2 instances
** Low latency performance and scalability
** More $$ than S3

Amazon Glacier: low cost archive
** Infrequent access
** VERY low cost

DATABASE

RDS (Relational Database Service): Database services
* MySQL, Oracle, SQL Server, Aurora
* Autoscaling
* Support backup

DynamoDB: NoSQL keystore DB
* Amazon's version of MongoDB
* Milliseconds latency

ElastiCache: In-memory cache for fast performance

Amazon RedShift: petabyte scale data warehouse
* SQL based, useful for large scale OLAP reporting

STREAMING PROCESSING
EMR (Elastic Map Reduce): Amazon's version of Hadoop

Amazon Kinesis: Real-time ingestion & processing of streaming data
* Ingest, process, and analyze real-time data like application logs, website clicks, Internet of Things telemetry

AWS Lambda: server-less computing, allowing you to run code w/o provisioning or managing servers, real time, event driven processing

Amazon Machine Learning: build ML models through wizards and run against data stored in S3, RedShift, or RDS.  Up to 100GB

Comparisons Between Major Cloud Providers:

============================================AWS==============================AZURE=======================================GOOGLE CLOUD

VMs============================EC2 + Container Service==================VMs, Azure Kubernetes===============Compute Engine/Kubernetes Engine

Event driven====================Amazon Lambda===========================Functions=====================================Cloud Functions
compute

Realtime processing=========Amazon Kinesis===========================Event hubs/Apache Storm=====================Cloud Dataflow

Hadoop====================Elastic MapReduce=========================HDInsight======================================Cloud Dataprpc

ML========================Amazon ML, TensorFlow===================Azure ML========================================Cloud ML, Cloud Deep Learning

Data storage=================S3======================================Storage=============================================Cloud Storage

Archiving=================AWS Glacier=================================Azure Backup===================================Storage Nearline

DB as Service===========Amazon RDS, Aurora========================Azure SQL DB, Cosmos DB====================Cloud SQL/DF

NoSQL==================DynamoDB=====================================Table Storage==================================Cloud Datastore

Warehousing============AWS RedShift=================================SQL DW=========================================Big Table/BigQuery

Working with RDDs in Spark
---------------------------------------------

Spark Deployment Nodes

* Local mode: run everything in a single JVM (useful for testing or demo)
* Cluster mode:
** Standalone: Spark manages its own cluster, simple/easy to setup
** YARN: Using YARN as cluster manager
** Mesos: Using Apache Mesos as cluster manager

Spark Clusters

* Each Spark application has driver program, using SPARKCONTEXT to communicate with cluster manager or local threads
** Spark program we write is driver program
* EXECUTOR is created on each worker node, runs Spark tasks and interacts with external storage (HDFS, S3, etc)
* RDD are distributed among worker nodes

Executors and Partitions

* 4-6 cores per executor to maximize HDFS i/o throughput

* Partitions rule of thumb: 128MB per partiion
** Don't have too big partitions, job may failr due to 2GB shuffle block limit
** Don't have too few partitions, job will be slow and won't make use of parellelism

Spark Context

* Spark program cretaes SparkContext objects
** Establishes connection with Spark execution environment
** Tells Spark how/where to access cluster
** Required for every Spark application
** Created in Spark Shell as var
** Manually created in standalone program

* Use SparkContext to create RDDs, access Spark services, run jobs

* Every Spark app requires SparkContext object, main entry point to Spark API
* Spark Shell provides preconfigured Spark Conntext called scalable
sc.appname

Resilient Distributed Datasets

* RDDs are part of core Spark
** Resilient: If data in memory is lost, it can be recreated
** Distributed: processed across the cluster

* Characteristics
** Immutable once constructed (cannot be changed)
** Track lineage information to efficiently recompute lost data
** Unstructured
*** No schema
*** Not table-like; cannot be queried using SQL like transformations like WHERE or SELECT
*** Convert unstructured or semi-structured data into structured form

Content of RDDs

* RDDs can hold any serializable type of element
** Primitive types: integers, characters, booleans
** Collections: strings, lists, arrays, tuples, dicts
** Scala/Java objects
** Mixed types

* Some RDDs have additional functionality
** Pair RDDs: key-value pairs
** Double RDds: numeric data

Constructing RDDs

* RDDs can be built using:
** files in HDFS or storage system
** Transforming a different/existing RDD
** Paralleliznig existing Python collections(lists)

Creating RDDs from Collections
* sc.parallelize(collection)
** Testing
** Generating data programmatically
** Integrating

Python Collection
>>>> sc.range(1, 10, 2, 4).collect() [last argument is number of paritions]
Create a collection from a range of numbers 1 through 10, skipping every two numbers, using 4 partitions

Creating RDDs from Files (1)
* sc.textFile
** Accepts single file, wildcard list of files, or CS list of files
** sc.textFile("myfile.txt")
** sc.textFile("mydata./*.log")

* Each line in file(s) is separate record in RDD.

* sc.hadoopFile or cs.newAPIHadoopFile to read other formats

Cont.
* Files are referenced by absolute or relative URI
•file:/home/training/myfile.txt --a file on a local host under /home/training\
•file:///c:/Users/John/documents/myfile.txt --a file on C:/Users... (in windows systems)
•hdfs://localhost:8020/loudacre/myfile.txt – a file on the HDFS cluster at host “localhost” at port 8020 under directory /loudacre
•s3n://bucket/directory/filename.ext

Cont.
* textFile maps a line in file to separate RDD element
** Only works with line-delimited text files

Cont.
* From HDFS, text file, Hyper table, S3, Hbase, etc, dir, global wildcard
>>> distFile = sc.textFile("README.md", 4)
>>>> distFile
MappedRDD[2] at textFile at Native blah blah
-*** RDD distributed in 4 partitions
*** Elements are line of input
*** LAZY EVALUATION - no execution happens at this point

File-Based RDDs
* sc.textFile maps each line in file to separate RDD element
** What about XML or JSON format?

* sc.wholeTextFile(dir)
** Maps entire contents of each file in dir, to single RDD element
** Only works for small files (must fit in memory)

Anonymous Functions
* RDD ops involve anonymous functions

* Python: lambda functions
** Restricted to single expression and not re-used

* Scala: anonymous function syntax with "=>"

val xboxRDD = auctionRDD.filter(line=>line.contains("xbox"))

Operations w/ RDDs
* TRANSFORMATIONS and ACTIONS
* Trans are lazy (not computed immediately)
* Transformed RDD executed when actions run on it

Cont.
* Actions: returns local dataset or value (non-RDD)
** count
** take(n)
** collect()

* Transformations: generate new RDDs based on current one
** filter
** map
** reduceByKey

Commonly Used Trans.
* map: returns new RDD by applying func to each element of source
* filter
* groupByKey: returns dataset (K, iterable<V>) pairs on dataset of (K, V)
*reduceByKey: returns (K, V) where value for each key is aggregated given reduce function
* flatMap
* distinct

** map(lambda line: line.upper()) - return all upper case values for every line
** filter(lambda line: line.startswith("I") - return all lines starting with I

sc.textFile(file) \
.flatMap(lambda line: line.split()) \
.distinct()

returns all distinct words in file, one at a time

Multi-RDD Transf.

** INTERSECTION: create new RDD with all elements in both original RDDs
** UNION: add all elements of two RDDs into single new RDD
** ZIP: pair each element of first RDD with corresponding element of second

Commonly Used Actions
** count(): return number of elements in dataset
** reduce(func): aggregate elemnts using the specific function
** collect(): return all elements of dataset as array to driver program
** take(n): return array with first n elements
** first(): return first element
** takeOrdered(n, [ordering]): returns first n elements of RDD using custom operator

>>> rdd = sc.parallelize([1, 2, 3])
>>> rdd.reduce(lambda a, b: a* b)
Value: 6

>>> rdd = sc.parallelize([5, 3, 1, 2])
>>> rdd.takeOrdered(3, lambda s: -1 * s)
Value: [5,3,2] (go descending order)

Other RDD Ops
* first: return first element of RDD
* foreach: apply function to each element in RDD
* top(n): return largest n elements using natural ordering

* sample: create new RDD with sampling of elements [trans]
* takeSample: return array of sampled elements [action]

* statistical functions run on DOUBLE RDDs Only
** mean, sum, variance, stdev

Lazy Execution
* RDD = sc.textFile(filename)
** No computation has happened yet.

* RDD = sc.textFile(filename)
* newRDD = RDD.filter(...)
** No computation has happened yet.

* RDD = sc.textFile(filename)
* newRDD = RDD.filter(...)
* newRDD.count()
** Data is read into memory, RDD and newRDD will be created.  After getting result (count), RDDs no longer in memory

Caching
lines = sc.textFile("....", 4)
comments = line.filter(isComment)
print lines.count(), comments.count()

* Spark will read source data twice
** 1st time - lines.count()
*** read data, sum within partitions, combine sums in driver

**2nd time - comments.count()
*** read data (again), filter & sum within partition, combine sums in driver

Cont.
lines = sc.textFile("....", 4)
lines.cache() # save, don't recompute
comments = line.filter(isComment)
print lines.count(), comments.count()

* Reading is a common step in two processes.  Use cache() to avoid re-computing.

SPARK SQL

* Spark SQL: module for structured data processing, built on top of core Spark

* Provides:
** DataFrame API - library for working with data as tables
** DataFrames containing Rows and Columns
** Catalyst optimzer - extensible optimization framework
** SQL Engine and CLI

Why Spark SQL
* Complex data manip & analytics
* Integration with other data systems / APIs
* ML
* Streaming/long running apps

SparkSession
* SparkSessions allow apps to create DFs from existing RDDs, Hive tables, or Spark data source
* SparkSession spark is created in spark shell

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MyApp") \
.config("SomeOption", "SomeValue").getOrCreate()

DataFrame

* DataFrame is main abstraction in Spark SQL
** Distributed collection of named columns
** Equivalent to columnar table in relational db

* DataFrame & RDDs
** DataFrame corresponds to RDD of Row objects

from pyspark.sql import Row

data = [('Ankit', 25), ('Jeff', 22), ('Saurabh', 20)]
rdd = sc.parallelize(data)
rowRdd = rdd.map(lambda x: Row[name=x[0], age=int(x[1])))

df = spark.createDataFrame(rowRdd)

Create DFs from RDDs
* Create DFs from existing RDDs, Hive tables, Spark dsources
** spark.CreateDataFrame(rdd, schema=None)

* Schema can be list of column names or StructType
** specify column names only
*** df = spark.createDataFrame(rdd, ['name', 'age'])

from pyspark.sql.types import *

schema = StructType([
	StructField("name", StringType(), False),
	StructField("age", IntegerType(), True)
])

df =spark.createDataFrame(rdd, schema)

Create DFs from Spark Data Sources
* Text files: CSV, JSON, plain text
* Binary format: Parquet, ORC
* Tables: Hive metastore, JBDC

Reading Data
* spark.read returns DataFrameReader object
* DataFrameReader settings to specify how to load data
** .format(source): json, parquet, csv, jdbc
** .options: header, inferSchema, delimiter, url
** .option(key, value): options one by one
** .schema: specify input schema

* Create DF based on data source
** load(): loads data from file/files
** table(): load table from Hive table

DF Reader Convenience Functions
* Call format specifci load function
** Shortcut instead of setting format and using load
** csv, json, orc, parquet, text, table, jdbc

spark.read.option("header", "true").format("csv").load("myFile.csv")
spark.read.csv("myFile.csv, header=True)

* Relative URL (default file system)
** myfile.json (HDFS home dir on VM - not true?)

* Absolute URI
** hdfs: //host/loudacre/myfile.json (HDFS)
** file:/home/cloudera/myfile.json (local host)

accountsDF = spark.read.format('jdbc').options( \
url = "jdbc: mysql://localhost/mybd?user= &password =", dbtable="accounts").load()

**** AVOID DIRECT ACCESS TO DBs in PRD, may overload the DB or be interpreted as service attacks****
--- Use sqoop to import instead

Save A DataFrame

DataFrameWriter
* Saves data to data source like table or set of files
*** the default save is parquet as a DIR
* Methods
** format: data source type
** mode: error, overwrite, append, or ignore
** partitionBy: stores data in partitioned dirs in form of column = value (like Hive)
** option: properties for target data source
** save: save data as file in specified dir
** saveAsTable: saves data to Hive metastore table (default table location/user/hive/warehouse)

myDF.write. \
	mode("append"). \
	option("path", "/loudacre/mydata"). \
	saveAsTable("default.my_table")
	

Register DF As Table
* Register DF as temp table using given name
* Then use that table in subsequent SQL queries

peopleDF.createOrReplaceTempView("people")
spark.sql("SELECT * FROM people WHERE name LIKE \"A%\" ".show()

* Lifetime of temp table is tied to SparkSession
* createGlobalTempView to create references across spark sessions

DataFrame Operations

* Meta operations: operate on meta data and not data itself
** printSchema

* Queries: create a new DF
** DFs are immutable
** Queries are like RDD transformation
** Qs are lazily evaluated
** Qs can be chained like transformations

* Actions: return data to Drive
** Trigger "lazy" exectuion of qs
** show()

MetaOps
* printSchema(): display schema as visual tree
* columns: array containing column names
* dtypes: array of (column name, type) pairs
* explain(): print debug info about DF to console
* createTempView(): registers DF as temp view using given name
* toDF(*cols): returns new DF with specified column names
* cache(): persist DF to disk or memory

DF Actions
* collect(): return all rows as array of Row objects
* count(): number of rows
* first/head(): first row
* show(n): print first n rows 
* take(n): returns first n rows as array of Row objects

** head() works differently from pandas, returns list of Row objects

Common Queries
* describe(): summary stats of columns
* select(cols): select a set of cols
* groupBy(col1, col2): group DF using specifed columns to run aggregations
* filter(conditionExpr): filters based on SQL expression
* distinct(): new DF containing only unique rows
* limit(n): new DF with first n rows of DF
* sort(cols); orderBy(cols): return DF sorted by specific columns
* join(other, joinExpr, joinType): join DF with second DF using join expression - inner, outer, left_outer)

DataFrame Column Methods
* df.name.contains("smith"): contains "smith"
* df.name.like("A%"): sql like
* df.name.substr(1,3).alias("short_name"): first three letters of name
* df.name.isin("Bob", "Mike")

* df.age.cast('string').alias('age_as_string')

* df.height.isNull()

* df.orderBy(df.name.desc())

* Column operations like avg, datediff, lower, rand, explode

import pyspark.sql.functions as facilities
df.select(f.explode(f.split(df.field, ",")))

Manipulating DFs
* withColumn(colName, col): return new DF adding or replacing column
* withColumnRenamed(existing, new): return new DF renaming existing column
* sample(): take sample from DF
* sampleBy(): return stratified sample
* replace(from, to, subset): return DF doing search and replace
* fillna(value): fill null values with new value
* drop(col): remove col
* dropDuplicates(subset): de-dup rows

** peopleDF.withColumn("age2", peopleDF.age + 2)

** peopleDF.withColumnRenamed("age2", "age_new")

** peopleDF.select("age", "gender").dropDuplicates().show()

** peopleDF.fillna(0, "age").show()
** peopleDF.na.fill()

** peopleDF.drop('age')

Aggregation and Windowing

* groupBy takes one or more column names/references
** returns GroupedData object

** count
** max and min
** mean (avg)
** sum
** pivot
** agg (additional agg functions)

** iris.groupBy().avg('petalLength').show()

Other agg functions
** countDistinct: number of unique items in group
** approx_count_distinct: faster than normal count
** stddev: standard deviation of values
** var_sample/var_pop: variance for values
** covar_sample/covar_pop: covariance
** corr: correlation of group of vals

Join DFs

* df.join(df2, joinExpr, joinType)

** joinExpr can be:
*** string for join column name ("ssn")
*** list of column names ['firstname', 'lastname']
*** join express:
****** df.id == df2.id
****** [df.fname == df2.fname, df.lname==df2.lname]

** joinType: inner, outer, left_outer, right_outer, cross, left_semi, left_anti

Interact with Tables and Views

Run SQL Queries

* SQL queries and DF transfs are equivalent
** myDF = spark.sql("SELECT * FROM people where pcode = 90420")
** myDF = spark.read.table("people").where("pcode=94020")

Query Files

spark. \
	sql("SELECT * FROM parquet. '/path/to/my.parquet' WHERE firstName LIKE '%A' ")
	

Create Views

* Query a view - perform SQL query on DF or Dset

* Views are temporary
** REGULAR: only used in single Spark sessions
** GLOBAL: shared between multiple Spark sessions in single spark app

* Creating a view
** DF.createTempView(view-name)
** DF.createOrReplaceTempView(view-name)

spark.read.load("/path/my.parquet"). \
	select("firstName", "lastName"). \
	createTempView("user_names")
	
spark.sql( \
	"SELECT * FROM user_names WHERE firstName LIKE 'A%" ). \
	show()
	

Qeury Tables with SQL

* Query Hive metastore tables

DataFrames and RDDs

* DFs are built on RDDs
** Base RDDs contain Row objects
** Use rdd to get underlying RDD

peopleRDD =peopleDF.rdd

* Row RDDs have all standard Spark actions/transfs

* Python
** row.age: return age column value from row

Scala
**row(0): return element in first column
** row(1): return element in second column

** Apache Impala
*** Better performance than Spark SQL
*** Mature, highly optimized, low latency
*** Robust security with Apache Sentry
*** Best for: interactive/ad hoc queries, data anlysis, integration with visual analytics like Tableau
*** Typical job: seconds or less

** Apache Hive (running on MR, Tez or Spark)
*** Uses either Spark or MR
*** Hive on spark has much better performance
*** Very mature, high stability and resilience
*** Best for ETL processing
*** Typical job is minutes to hours


** Spark SQL API (SparkSession.sql)
*** Mixed procedural and SQL applications
*** Rich ecosystem of related APIs for ML, data streaming, statistical computation
*** Catalyst optimzer for good perfom
*** Supports Python, common language for D Scientists
*** Best for: Complex data manip, integration with other data systems/APIs, ML, streaming and long running apps
*** Typical job: 

Essential Points

* Spark SQL is Spark API for handling structured and semi-structured data
* Entry point is Spark Session: spark
* DataFrames are key unit of data
** DFs are based on underlying RDD of Row objects
** DF query methods return new DFs; similar to RDD transformation
** Full Spark API can be used with Spark SQL data by accessing underyling RDD
** Spark SQL is not replacement for database or specialized SQL engine

* Spark SQL is most useful for ETL or incorporating structured data into other applications

SPARK MACHINE LEARNING

Intro to Spark MLlib

* Spark MLlib is Spark's scalable ML library
** Contains common learning algorithms / utilies, including classification, regression, clusterings, collaborative filtering, dimensionality reduction, underlying optimization primitives
** Popular for large scale ML
** Scala, Python, Java, R APIs

* Two packages
** org.apache.spark.mllib / pyspark.mllib (older API based on RDD)
*** RDD-based API, expected to be retired in Spark 3

** org.apache.spark.ml / pyspark.ml
*** DF based API, support pipeline
*** Feature parity with RDD based APIs (Spark 2.3x - Feb 2018)

Why DF based API

* pyspark.ml
** DFs provide user friendly API
** DF API support practical ML pipelines
** Support DF data sources (text, csv, json, parquet, image)
** Support SQL/DF queries
** Tungsten & Catalyst optimization
** Feature parity with older RDD based APIs

Use Cases

* 3M Health Info Systems
** Use ML to predict patient outcomes

* Customer 360 @ Toyota
** Monitor social media interactions
** Classify social media interactions into buckets
*** Campaign opinions
*** Customer feedback
*** Product feedback

Why Spark MLLib

* Built on APACHE SPARK, fast general engine for large scale data processing

* Inherits merits of Spark
** FAST compared to other libraries specialized in ML
** SIMPLICITY: few lines of code can do a lot
** MULTI-LANGUAGE: scala, python, R
** INTEGRATION: storage systems
** UNIFIED STACK: special purpose ML package may be better, but cost of constant switching is high (diff language, data formats)

MLLIB DATA TYPES AND FILE FORMATS

MLLib data types: Vectors

* label: represented by double column
** BINARY classification, it should be 0.0 or 1.0
** MULTICLASS classification: 0.0, 1.0, 2.0
** REGRESSION: float or double types

* features: represented by vector column
** In unsupervised learning, only features column is needed

* DENSE vs SPARSE vectors
** Dense vector: regular array of doubles
** Sparse vector: two parallel arrays, one for indicator of elements that are present, other for double values of elements

Create Dense & Sparse Vectors

* Vectors are defined in Spark's MLLib linalg module

from pyspark.ml.linalg import Vectors

dv = Vectors.dense([1.0, 0.0, 3.0])
sv = Vectors.sparse(3, [0, 2], [1.0, 3.0])
sv2 = Vectors.sparse(3, {0:1.0, 2:3.0})

Why Use Sparse Vectors

* Saves on storage cost and speeds up computation

						Dense							Sparse
Storage			47 GB							7 GB
Time				240s						58s

Create ML DFs in Programming Way

* Directly provide rows; useful for demos and testing
** Define array of (FEATURES) or (LABEL, FEATURES) where FEATURES are sparse/dense vectors

from pyspark.ml.linalg import Vectors 
data=[(1.0,Vectors.sparse(4,[(0,1.0),(3,-2.0)])),
(0.0,Vectors.dense([4.0,5.0,0.0,3.0])),
(1.0,Vectors.dense([6.0,7.0,0.0,8.0])),
(1.0,Vectors.sparse(4,[(0,9.0),(3,1.0)]))]
#convert to DataFramewith col names
df=spark.createDataFrame(data,["label","features"])
df

+-----+--------------------+
|label|            features|
+-----+--------------------+
|  1.0|(4,[0,3],[1.0,-2.0])|
|  0.0|   [4.0,5.0,0.0,3.0]|
|  1.0|   [6.0,7.0,0.0,8.0]|
|  1.0| (4,[0,3],[9.0,1.0])|
+-----+--------------------+

Load Dat from LIBSVM source files

* LIBSVM is compact text format for encoding data (usually training sets)
** Widely used in MLLib to represent sparse feature vectors
** Layout is:
*** class_label index1:value1 index2:value2 ...
*** numeric indices represent features; values are separated
*** MLLib expects to start class labeling at 0
*** Feature indices are one-based in ascending order (1, 2, 3, etc)
*** Feature not present in record is omitted

df = spark.read.format("libsvm").load("sample_libsvm_data.txt")

Transform Existing DF

* Spark MLLib provides VectorAssembler that combines a given set of columns into single vector column

TRANSFORMER, ESTIMATOR, AND PIPELINE

Overview of DF Based APIs

* Machine Learning tasks consist of series of steps
* Steps can viewed as pipeline thru which data travels
* Training and prediction typically follow same data processing steps

MLlib Abstraction of Data, Models, Algorithms
* MLLib aims to standardize interface for ML pipelines

* MLlib main attractions:
** DataFrame: Spark MLlib dataset type (with feature vector column)
** Transformer: Transforms one DF into another
** Estimator: Runs algorithm on data set to fit a model
** Pipeline: Chains multiple steps to define ML workflow

Transformers

* Transformers take DF as input, return a new DF
** Append one or more columns to input DF
** Abstraction for FEATURE TRANSFORMATION and LEARNED Models
*** Feature transformer: standardize a numerical field
*** Learned model: read features column, predict the label for each feature vector, and output new DF with predicted label as new column


* All transformers implement transform(df) method

* When defining transformer, need to provide:
** inputCol: for name of column to be transformed
** outputCol: for name of column to store transformed data in output DF

from pyspark.ml.feature import Tokenizer

sentenceDataFrame=spark.createDataFrame([
	(0,"Hi I heard about Spark"),
	(1,"I wish Java could use case classes"),
	(2,"Logistic,regression,models,are,neat")
],["id","sentence"])

tokenizer =Tokenizer(inputCol="sentence",outputCol="words") ## initalize Tokenizer

tokenized =tokenizer.transform(sentenceDataFrame)
tokenized.select("sentence","words").show(truncate=False) ## use it to transform data

Estimators

* Estimator takes DF as input and produces LEARNED Model
** Learning algorithms are implemented as Estimators
** Learning mdoel is TRANSFORMER

* All estimators implement fit(df) method
** Estimators initialized with specific set of parameters to be used when algorithm is run
** fit method runs algorithm on provided data (df)
** Result is instance of learned model (transformer) for that particular algorithm

Relationship b/w Transformer & Estimator

* Both have ability to transform data
* Key difference lies in whether "training" is required
** Estimator requires training (using fit method)
*** fit produces a model (which is transformer)
*** can be thought of a transformer that requires training before use
*** scaler.fit(df).transform(df)

** Transformer does not require training
*** can be directly used
*** tokenizer.transform(df)

Pipelines

* Pipeline represents series of steps in ML workflow
** Each step is either transformer or estimator
** Takes a DF as input and produces PipelineModel as output
** Pipeline is an ESTIMATOR; PipelineModel is transformer

* Pipelines simplify ML workflow
** W/o pipeline, you must manually carry out each step for both training and prediction.
** W/ pipeline, you can define sequence of steps onces and re-use it for both training and prediction

MLLIB FEATURE ENGINEERING APIs

Support Feature Engineering APIs

TF-IDF: calculates importance of words in given body of text
Word2Vec*: converts words in text to vectors to enable numerical calculation of similarity
Tokenizer: break text into individual words
StopWordsRemover: remove stop words from seq of strings
NGram: take seq of strings and computes n-grams
StandardScaler*: scale/center for features to standard dev one and zero mean
Normalizer: scales features to have length 1 when viewed as vector
MinMaxScaler*: rescaling each feature to specific range ([0,1])
SQLTransformer: implement tranfs defined by SQL statement
Imputer*: complete missing values in dset, using mean/median
OneHotEncoderEstimator*: encode single categorical variable into set of binary continous features, one for each category of original variable
StringIndexer*: Encode as tring representation of categorial variable into integer value
VectorAssembler: Aggregates DF columns into single column containing vector
VectorIndexer*: Indexing categorical feature columns in dataset of vector. Turn ds of vectors into some with continous/categorical features

StringIndexer: encode string categorical features into numerical features

* Many algorithms only take numerical features

* StringIndexer(inputCol, outputCol)
** ESTIMATOR that encodes categorical features
** Operates on single column of Spark DF
*** Requires separate StringIndexer for each categorical column
** Requires fit before transformation

* IndexToString(inputCol, outputCol)
** TRANSFORMER that transforms indexed column back to strings using info in column meta data
** Implements transform()
** Reverse StringIndexer

**** StringIndexer uses frequencyDesc order, CA will have code of 6.0 because it is ranked 7 in frequencyDesc

VectorAssembler

* VectorAssembler is TRANSFORMER that combines given list of columns into single vector column
** Useful for combining raw features and features generated by different feature transformers into single feature vector
** Accepts all numeric types, boolean, and vector type
** Values of input column are concatenated into vector in given order

OneHotEncoderEstimator

* OneHotEncoderEstimator: maps categorical feature, represented as label index, to binary vector
** Encoding allows algorithms which expect continous features, such as Logit Regress, to use categorical features
** For string type input, common to encode categorical features using StringIndexer first

EVALUATION & HYPER PARAMETER TUNING

Evaluation and Cross Validation in Spark MLLib

* ml.evaluation.MulticlassClassificationEvaluator(predictionCol='prediction',
	labelCol = 'label', metricName ='f1'):
** Other metrics: weightedPrecision(weightedRecall|accuracy)

* ml.evaluation.BinaryClassificationEvaluator(rawPredictionCol='raw prediction',
	labelCol = 'label', metricName ='areaUnderROC')
** Other metric: "areaUnderPR"

* ml.tuning.CrossValidator(estimator, estimatorParamMaps, evaluator, numFolds=3)
** K-fold cross validation
** CrossValidator is also estimator, implements fit method

Hyper Parameter Tuning: TrainValidationSplit

* Cross validation is well established method for choosing paramaters more statisically sound

* Spark provides another hyper-parameter tuning estimator called TrainValidationSplit that is less expensive but may not have reliable results

* ml.tuning.TrainValidationSplit(estimator, estimatorParamMaps, evaluator, trainRation = 0.75, parallelism=1)
** TrainValidationSplit only evaluates each combo of params once, as opposed to k times in CrossValidation
** It does split of dataset based on trainRation

APPENDIX

Algorithms and Tools in MLlib (pyspark.ml)

* .classification: logit regress, SVM (linearSVC), Naive Bayes, Multilayer Preceptron
* .tree: decision tree, random forest, gradient boosted trees
* .regression: linear regress, ridge, Lasso, Accelerated failure Time Survival, Generalized Linear Regrss
* .recommendation: alternating least squares (ALS)
* .clustering: k-means, bisecting k-means, Gaussian mixture
* .fpm: FPGrowth (mining freq itemsets), PrefixSpan (mining freq seq patterns)

* .linalg: dense/sparse vector/matrix
* .feature: HashTF, IDF, Word2Vec, Normalizer, Standard, 50 more
* .stat: ChiSqTest, Correlation, Summarizer
* .evaluation: BinaryClassificationEvaluator, MulticlassClassificationEvaluator
* .param: Param, Params
* .tuning: ParamGridBulder, CrossValidator, TrainValidationSplit