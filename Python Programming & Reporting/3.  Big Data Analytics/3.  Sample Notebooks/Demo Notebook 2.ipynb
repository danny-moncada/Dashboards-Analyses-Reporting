{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Short Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128.1.1.2 shopping_basket.html\n",
    "128.1.1.3 checkout.html\n",
    "128.1.1.2 search.html\n",
    "128.1.1.2 product.html\n",
    "128.1.1.3 shopping_basket.html\n",
    "128.1.1.4 homepage.html\n",
    "128.1.1.5 product.html\n",
    "128.1.1.8 search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _map_ function\n",
    "\n",
    "* __input__: \"128.1.1.2 shopping_basket.html\", \"128.1.1.3 checkout.html\", \"128.1.1.2 search.html\", \"128.1.1.2 product.html\", \"128.1.1.3 shopping_basket.html\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __output__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\"shopping_basket.html\",1), (\"checkout.html\",1), (\"search.html\",1), (\"product.html\",1), (\"shopping_basket.html\",1), (\"homepage.html\",1), (\"product.html\",1), (\"search.html\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __logic__: Split each row (as a STRING) of the web log into two parts, the server IP address & the web page visited.  Then for each web page in the list, output the (web page, 1) pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _reduce_ function\n",
    "\n",
    "* __input__: (\"shopping_basket.html\", (1,1)), (\"checkout.html\",(1)), (\"search.html\",(1,1)), (\"product.html\",(1,1)), (\"homepage.html\",(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __output__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_basket.html 2\n",
    "checkout.html 1\n",
    "search.html 2\n",
    "product.html 2\n",
    "homepage.html 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __logic__: Takes list of values for every web page, and sum them to get the number of times each page was requested in the web log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. List three main advantages of data lakes based on big data technologies and explain why.  What are the main disadvantages (at least two) and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the learnings from the course so far, there are many advantages to having data lakes for doing analysis and obtaining information.  My arguments for:\n",
    "\n",
    "1.  A traditional data mart or data ware house is like a store of bottled water.  A \"data lake\" is a large body of water in a more natual state.  The contents to that data lake stream in from MULTIPLE sources to fill that lake, and let various users, analysts, and data scientists examine its contents, \"dive in\" and do exploratory analysis, or \"take samples\" and do an analysis on a subset of data.  You can serve a huge population of users across different countries, time zones, offices, etc.  A data warehouse/RDBMS is pretty limited in what it can take in for input, the scope of what it can produce, and distributing that information for a large population.\n",
    "<br/>\n",
    "\n",
    "2.  You have more analytic capablities and can work with more data types than you normally could with just a SQL database/ware house.  The MapReduce framework of (key, value) pairs accommodates different kinds of structured/unstructured data.  For example, if you work for a retailer than tracks tradition transactional data like daily sales while at the same time capturing web traffic information, having a tool like Hadoop is extremely helpful because you can store the mixture of structured and unstructured data side by side.  As a compnay, having this flexibility is extremely important as more data (five exabytes every two days) is being generated across the world.\n",
    "<br/>\n",
    "\n",
    "3.  Traditional storing of big data is really expensive, and normally we have to Extract, Transform, and Load data in order to summarize and archive results in a data warehouse.  However, you lose the _detail_ in that data when you aggregate it, and there are a lot of \\*nuggets* to derive from that information.  Data lakes allow us to store that data cheaply and more effectively.  Using Hadoop provides analysts and data scientists reliable distributed storage and a framework for parallel processing at a low cost.  We can actually process this data as we're consuming it, when previously we could not.  Another point that I have learned in the course thus far is it's not the best algorithm that wins, it's whoever has the most data.  A data lake helps companies aggregate that information more effectively.\n",
    "<br/>\n",
    "\n",
    "However, this does not mean this technology does not have disadvantages.  Here are some examples:\n",
    "\n",
    "1.  With the sheer volume of data that could be placed in the data lake, if it is not indexed or sorted correctly, it will be almost impossible to find anything.  Data governance is extremely important in that sense; if there isn't a stable process to confirm how the data is getting placed into the centralized location, it will quickly become a mess or what is known as a \"data swamp\".\n",
    "<br/>\n",
    "\n",
    "2.  The latency of the technology is extremely slow.  It is not meant businesses or companies that deal with strictly transactional data because querying that type of information with a data lake would be very cumbersome.\n",
    "<br/>\n",
    "\n",
    "3.  Analysts and users of the tool need to be made aware of the proper way to access information.  I learned in this midterm that writing a query that would normally take no time in SQL takes a huge amount of time in Hive because it is structured differently.  A lack of general awareness of how the infrastructure, technology, and languages work together makes it a significant challenge to teach others how to use it properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Hands on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Get the data into the system and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "[cloudera@quickstart ~]$ mkdir midterm\n",
    "\n",
    "[cloudera@quickstart ~]$ ls -l\n",
    "total 556\n",
    "drwxrwxr-x  2 cloudera cloudera   4096 Jul 20 11:39 midterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "[cloudera@quickstart ~]$ cd midterm\n",
    "[cloudera@quickstart midterm]$ pwd\n",
    "/home/cloudera/midterm\n",
    "[cloudera@quickstart midterm]$ wget http://idsdl.csom.umn.edu/c/share/counties.csv--2019-07-20 11:41:31--  http://idsdl.csom.umn.edu/c/share/counties.csv\n",
    "Resolving idsdl.csom.umn.edu... 134.84.138.46, 2607:ea00:101:480a:250:56ff:febb:e76b\n",
    "Connecting to idsdl.csom.umn.edu|134.84.138.46|:80... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 235544 (230K) [text/csv]\n",
    "Saving to: “counties.csv”\n",
    "\n",
    "100%[=====================================================================================================================================================================================================================================>] 235,544     --.-K/s   in 0.01s\n",
    "\n",
    "2019-07-20 11:41:31 (17.8 MB/s) - “counties.csv” saved [235544/235544]\n",
    "\n",
    "[cloudera@quickstart midterm]$ wget http://idsdl.csom.umn.edu/c/share/election16.csv\n",
    "--2019-07-20 11:41:43--  http://idsdl.csom.umn.edu/c/share/election16.csv\n",
    "Resolving idsdl.csom.umn.edu... 134.84.138.46, 2607:ea00:101:480a:250:56ff:febb:e76b\n",
    "Connecting to idsdl.csom.umn.edu|134.84.138.46|:80... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 790867 (772K) [text/csv]\n",
    "Saving to: “election16.csv”\n",
    "\n",
    "100%[=====================================================================================================================================================================================================================================>] 790,867     --.-K/s   in 0.09s\n",
    "\n",
    "2019-07-20 11:41:43 (8.81 MB/s) - “election16.csv” saved [790867/790867]\n",
    "\n",
    "[cloudera@quickstart midterm]$ ls -l\n",
    "total 1008\n",
    "-rw-rw-r-- 1 cloudera cloudera 235544 Nov  7  2017 counties.csv\n",
    "-rw-rw-r-- 1 cloudera cloudera 790867 Jul 19 21:34 election16.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -mkdir /user/cloudera/counties\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -ls /user/cloudera\n",
    "Found 10 items\n",
    "drwxr-xr-x   - cloudera cloudera          0 2019-07-20 11:45 /user/cloudera/counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -put /home/cloudera/midterm/counties.csv /user/cloudera/counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -ls /user/cloudera/counties\n",
    "Found 1 items\n",
    "-rw-r--r--   1 cloudera cloudera     235544 2019-07-20 11:49 /user/cloudera/counties/counties.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.\n",
    "\n",
    "## HDFS counties.csv file has 3195 lines.\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -cat /user/cloudera/counties/* | wc -l\n",
    "3195\n",
    "\n",
    "\n",
    "## Local host counties.csv file has 3195 lines\n",
    "\n",
    "[cloudera@quickstart midterm]$ wc -l counties.csv\n",
    "3195 counties.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -cat /user/cloudera/counties/* | head -200 > /home/cloudera/midterm/counties_sample.csv\n",
    "cat: Unable to write to output stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.\n",
    "\n",
    "## counties_sample.csv file is 14820 bytes.\n",
    "\n",
    "[cloudera@quickstart midterm]$ ls -l *sample*\n",
    "-rw-rw-r-- 1 cloudera cloudera 14820 Jul 20 12:03 counties_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.\n",
    "\n",
    "## Looks like a normal CSV, comma separated values\n",
    "\n",
    "[cloudera@quickstart midterm]$ head -10 *sample*\n",
    "0,United States,,318857056,308745538,23.1,14.5,50.8,77.4,13.2,1.2,53046,28.8\n",
    "1000,Alabama,,4849377,4779736,22.8,15.3,51.5,69.7,26.7,0.7,43253,22.6\n",
    "1001,Autauga County,AL,55395,54571,25.2,13.8,51.4,77.9,18.7,0.5,53682,20.9\n",
    "1003,Baldwin County,AL,200111,182265,22.2,18.7,51.2,87.1,9.6,0.7,50221,27.7\n",
    "1005,Barbour County,AL,26887,27457,21.2,16.5,46.6,50.2,47.6,0.6,32911,13.4\n",
    "1007,Bibb County,AL,22506,22915,21,14.8,45.9,76.3,22.1,0.4,36447,12.1\n",
    "1009,Blount County,AL,57719,57322,23.6,17,50.5,96,1.8,0.6,44145,12.1\n",
    "1011,Bullock County,AL,10764,10914,21.4,14.9,45.3,26.9,70.1,0.8,32033,12.5\n",
    "1013,Butler County,AL,20296,20947,23.6,18,53.6,53.9,44,0.4,29918,14\n",
    "1015,Calhoun County,AL,115916,118572,22.2,16,51.8,75.8,21.1,0.5,39962,16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Use Hive to create [election] database and [counties] table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.\n",
    "\n",
    "0: jdbc:hive2://> CREATE DATABASE election;\n",
    "OK\n",
    "No rows affected (10.631 seconds)\n",
    "\n",
    "0: jdbc:hive2://> USE election;\n",
    "OK\n",
    "No rows affected (0.264 seconds)\n",
    "\n",
    "0: jdbc:hive2://> SHOW DATABASES;\n",
    "OK\n",
    "+----------------+--+\n",
    "| database_name  |\n",
    "+----------------+--+\n",
    "| default        |\n",
    "| dualcore       |\n",
    "| election       |\n",
    "| tmdb_movies    |\n",
    "| tweets         |\n",
    "+----------------+--+\n",
    "5 rows selected (1.705 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.\n",
    "\n",
    "0: jdbc:hive2://> CREATE EXTERNAL TABLE counties (FIPS INT, AREA_NAME STRING, STATE_ABBREVIATION STRING, POPULATION INT, POPULATION2010 INT, AGE18 FLOAT, AGE65 FLOAT, FEMALEPCT FLOAT, WHITEPCT FLOAT, BLACKPCT FLOAT, \n",
    "                                                  NATIVEPCT FLOAT, INC INT, COLLEGEPCT FLOAT) \n",
    "            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "            WITH SERDEPROPERTIES (\"separatorChar\" = \",\", \"quoteChar\" = \"\\\"\") \n",
    "            LOCATION '/user/cloudera/counties/';\n",
    "OK\n",
    "No rows affected (0.936 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14.\n",
    "\n",
    "0: jdbc:hive2://> SELECT * FROM counties LIMIT 10;\n",
    "\n",
    "    \n",
    "## Table loaded successfully.\n",
    "+----------------+---------------------+------------------------------+----------------------+--------------------------+-----------------+-----------------+---------------------+--------------------+--------------------+---------------------+---------------+----------------------+--+\n",
    "| counties.fips  | counties.area_name  | counties.state_abbreviation  | counties.population  | counties.population2010  | counties.age18  | counties.age65  | counties.femalepct  | counties.whitepct  | counties.blackpct  | counties.nativepct  | counties.inc  | counties.collegepct  |\n",
    "+----------------+---------------------+------------------------------+----------------------+--------------------------+-----------------+-----------------+---------------------+--------------------+--------------------+---------------------+---------------+----------------------+--+\n",
    "| 0              | United States       |                              | 318857056            | 308745538                | 23.1            | 14.5            | 50.8                | 77.4               | 13.2               | 1.2                 | 53046         | 28.8                 |\n",
    "| 1000           | Alabama             |                              | 4849377              | 4779736                  | 22.8            | 15.3            | 51.5                | 69.7               | 26.7               | 0.7                 | 43253         | 22.6                 |\n",
    "| 1001           | Autauga County      | AL                           | 55395                | 54571                    | 25.2            | 13.8            | 51.4                | 77.9               | 18.7               | 0.5                 | 53682         | 20.9                 |\n",
    "| 1003           | Baldwin County      | AL                           | 200111               | 182265                   | 22.2            | 18.7            | 51.2                | 87.1               | 9.6                | 0.7                 | 50221         | 27.7                 |\n",
    "| 1005           | Barbour County      | AL                           | 26887                | 27457                    | 21.2            | 16.5            | 46.6                | 50.2               | 47.6               | 0.6                 | 32911         | 13.4                 |\n",
    "| 1007           | Bibb County         | AL                           | 22506                | 22915                    | 21              | 14.8            | 45.9                | 76.3               | 22.1               | 0.4                 | 36447         | 12.1                 |\n",
    "| 1009           | Blount County       | AL                           | 57719                | 57322                    | 23.6            | 17              | 50.5                | 96                 | 1.8                | 0.6                 | 44145         | 12.1                 |\n",
    "| 1011           | Bullock County      | AL                           | 10764                | 10914                    | 21.4            | 14.9            | 45.3                | 26.9               | 70.1               | 0.8                 | 32033         | 12.5                 |\n",
    "| 1013           | Butler County       | AL                           | 20296                | 20947                    | 23.6            | 18              | 53.6                | 53.9               | 44                 | 0.4                 | 29918         | 14                   |\n",
    "| 1015           | Calhoun County      | AL                           | 115916               | 118572                   | 22.2            | 16              | 51.8                | 75.8               | 21.1               | 0.5                 | 39962         | 16.1                 |\n",
    "+----------------+---------------------+------------------------------+----------------------+--------------------------+-----------------+-----------------+---------------------+--------------------+--------------------+---------------------+---------------+----------------------+--+\n",
    "10 rows selected (1.707 seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Use Hive to ingest and analyze election16.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.\n",
    "\n",
    "## Not delimited by commas (',') but by pipes ('|').  And there is a header row.\n",
    "\n",
    "[cloudera@quickstart midterm]$ head -10 election*\n",
    "\n",
    "county|state|fips|party|candidate|votes\n",
    "Autauga|AL|1001|Democrat|Bernie Sanders|544\n",
    "Autauga|AL|1001|Democrat|Hillary Clinton|2387\n",
    "Baldwin|AL|1003|Democrat|Bernie Sanders|2694\n",
    "Baldwin|AL|1003|Democrat|Hillary Clinton|5290\n",
    "Barbour|AL|1005|Democrat|Bernie Sanders|222\n",
    "Barbour|AL|1005|Democrat|Hillary Clinton|2567\n",
    "Bibb|AL|1007|Democrat|Bernie Sanders|246\n",
    "Bibb|AL|1007|Democrat|Hillary Clinton|942\n",
    "Blount|AL|1009|Democrat|Bernie Sanders|395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16.\n",
    "\n",
    "## Made a new dir in HDFS for elections data (probably not needed but it is easier to manage).  Verified file was copied over correctly.\n",
    "[cloudera@quickstart midterm]$ hadoop fs -mkdir /user/cloudera/elections\n",
    "[cloudera@quickstart midterm]$ hadoop fs -put /home/cloudera/midterm/elect* /user/cloudera/elections\n",
    "[cloudera@quickstart midterm]$ hadoop fs -cat /user/cloudera/elections/* | wc -l\n",
    "17480\n",
    "[cloudera@quickstart midterm]$ wc -l electio*\n",
    "17480 election16.csv\n",
    "\n",
    "## Created 2 tables, one to grab the CSV data, and the second using ORC storage format.\n",
    "\n",
    "0: jdbc:hive2://> CREATE TABLE election_csv (COUNTY STRING, STATE_ABBREVIATION STRING, FIPS INT, PARTY STRING, CANDIDATE STRING, VOTES INT) \n",
    "            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "            WITH SERDEPROPERTIES (\"separatorChar\" = \"|\") \n",
    "            LOCATION '/user/cloudera/election/' \n",
    "            TBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n",
    "OK\n",
    "No rows affected (0.452 seconds)\n",
    "\n",
    "## Checking to see if the CSV table loaded, it did not.\n",
    "\n",
    "0: jdbc:hive2://> SELECT * from election_csv LIMIT 1;\n",
    "OK\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "| election_csv.county  | election_csv.state_abbreviation  | election_csv.fips  | election_csv.party  | election_csv.candidate  | election_csv.votes  |\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "No rows selected (0.326 seconds)\n",
    "\n",
    "## LOAD the CSV table first\n",
    "0: jdbc:hive2://> LOAD DATA LOCAL INPATH '/home/cloudera/midterm/election16.csv' INTO TABLE election_csv;\n",
    "Loading data to table election.election_csv\n",
    "Table election.election_csv stats: [numFiles=1, totalSize=790867]\n",
    "OK\n",
    "No rows affected (1.327 seconds)\n",
    "\n",
    "## Confirmed that data loaded this time.\n",
    "0: jdbc:hive2://> SELECT * from election_csv LIMIT 1;\n",
    "OK\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "| election_csv.county  | election_csv.state_abbreviation  | election_csv.fips  | election_csv.party  | election_csv.candidate  | election_csv.votes  |\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "| Autauga              | AL                               | 1001               | Democrat            | Bernie Sanders          | 544                 |\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "1 row selected (0.31 seconds)\n",
    "\n",
    "## Create the election_orc table with a different format, and load it using the CSV table created above.\n",
    "\n",
    "0: jdbc:hive2://> CREATE TABLE election_orc (COUNTY STRING, STATE_ABBREVIATION STRING, FIPS INT, PARTY STRING, CANDIDATE STRING, VOTES INT) STORED AS ORC;\n",
    "OK\n",
    "No rows affected (0.239 seconds)\n",
    "\n",
    "## Confirm table is empty after being created.\n",
    "0: jdbc:hive2://> SELECT * FROM election_orc;\n",
    "OK\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "| election_orc.county  | election_orc.state_abbreviation  | election_orc.fips  | election_orc.party  | election_orc.candidate  | election_orc.votes  |\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "No rows selected (0.276 seconds)\n",
    "\n",
    "## Load the table using csv table - SUCCESS!\n",
    "\n",
    "0: jdbc:hive2://> INSERT INTO TABLE election_orc SELECT * FROM election_csv;\n",
    "Query ID = cloudera_20190720131212_a3c88aa2-87a1-4a29-8c56-e5d6872e301d\n",
    "Total jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Number of reduce tasks is set to 0 since there's no reduce operator\n",
    "19/07/20 13:13:01 [HiveServer2-Background-Pool: Thread-95]: WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
    "Starting Job = job_1563637283469_0001, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1563637283469_0001/\n",
    "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1563637283469_0001\n",
    "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
    "19/07/20 13:13:49 [HiveServer2-Background-Pool: Thread-95]: WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
    "2019-07-20 13:13:49,005 Stage-1 map = 0%,  reduce = 0%\n",
    "2019-07-20 13:14:20,890 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 11.54 sec\n",
    "MapReduce Total cumulative CPU time: 11 seconds 540 msec\n",
    "Ended Job = job_1563637283469_0001\n",
    "Stage-4 is selected by condition resolver.\n",
    "Stage-3 is filtered out by condition resolver.\n",
    "Stage-5 is filtered out by condition resolver.\n",
    "Moving data to: hdfs://quickstart.cloudera:8020/user/hive/warehouse/election.db/election_orc/.hive-staging_hive_2019-07-20_13-12-57_883_5457467358191804834-1/-ext-10000\n",
    "Loading data to table election.election_orc\n",
    "Table election.election_orc stats: [numFiles=1, numRows=17479, totalSize=60900, rawDataSize=6502188]\n",
    "MapReduce Jobs Launched:\n",
    "Stage-Stage-1: Map: 1   Cumulative CPU: 11.54 sec   HDFS Read: 794975 HDFS Write: 60985 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 11 seconds 540 msec\n",
    "OK\n",
    "No rows affected (86.313 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17.\n",
    "\n",
    "0: jdbc:hive2://> SELECT * FROM election_orc LIMIT 5;\n",
    "OK\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "| election_orc.county  | election_orc.state_abbreviation  | election_orc.fips  | election_orc.party  | election_orc.candidate  | election_orc.votes  |\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "| Autauga              | AL                               | 1001               | Democrat            | Bernie Sanders          | 544                 |\n",
    "| Autauga              | AL                               | 1001               | Democrat            | Hillary Clinton         | 2387                |\n",
    "| Baldwin              | AL                               | 1003               | Democrat            | Bernie Sanders          | 2694                |\n",
    "| Baldwin              | AL                               | 1003               | Democrat            | Hillary Clinton         | 5290                |\n",
    "| Barbour              | AL                               | 1005               | Democrat            | Bernie Sanders          | 222                 |\n",
    "+----------------------+----------------------------------+--------------------+---------------------+-------------------------+---------------------+--+\n",
    "5 rows selected (0.643 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.\n",
    "\n",
    "## 790867 bytes or 790.867 KB on local file system \n",
    "[cloudera@quickstart midterm]$ ls -l election*\n",
    "-rw-rw-r-- 1 cloudera cloudera 790867 Jul 19 21:34 election16.csv\n",
    "\n",
    "## 59.5 K!  Much smaller compared to the CSV file - literally 7.5% of the original size!\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -du -h /user/hive/warehouse/election.db\n",
    "59.5 K  59.5 K  /user/hive/warehouse/election.db/election_orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19.\n",
    "\n",
    "## The total three candidates by total votes were Hillary Clinton (Democrat), Donald Frump (Republican), and Bernie Sanders (Democrat).  Didn't really help us!\n",
    "\n",
    "0: jdbc:hive2://> SELECT candidate, party, SUM(votes) AS total_votes FROM election_orc GROUP BY candidate, party ORDER BY total_votes DESC LIMIT 3;\n",
    "Total MapReduce CPU Time Spent: 19 seconds 270 msec\n",
    "OK\n",
    "+------------------+-------------+--------------+--+\n",
    "|    candidate     |    party    | total_votes  |\n",
    "+------------------+-------------+--------------+--+\n",
    "| Hillary Clinton  | Democrat    | 14122335     |\n",
    "| Donald Trump     | Republican  | 12559572     |\n",
    "| Bernie Sanders   | Democrat    | 10332812     |\n",
    "+------------------+-------------+--------------+--+\n",
    "3 rows selected (144.886 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20.\n",
    "\n",
    "0: jdbc:hive2://> SELECT t1.candidate, t1.state_abbreviation, t1.total_votes, t1.total_votes / t2.total_pop * 100 AS percentage_of_votes FROM \n",
    "            (SELECT e.candidate, e.state_abbreviation, sum(e.votes) as total_votes FROM election_orc e GROUP BY e.candidate, e.state_abbreviation ORDER BY total_votes DESC) t1 \n",
    "            LEFT JOIN (SELECT c.state_abbreviation, sum(c.population) AS total_pop FROM counties c GROUP BY c.state_abbreviation ORDER BY total_pop) t2 \n",
    "            ON (t1.state_abbreviation = t2.state_abbreviation) \n",
    "            LIMIT 1;\n",
    "\n",
    "OK\n",
    "+------------------+------------------------+-----------------+----------------------+--+\n",
    "|   t1.candidate   | t1.state_abbreviation  | t1.total_votes  | percentage_of_votes  |\n",
    "+------------------+------------------------+-----------------+----------------------+--+\n",
    "| Hillary Clinton  | CA                     | 1940580         | 5.0011726048579339    |\n",
    "+------------------+------------------------+-----------------+----------------------+--+\n",
    "1 row selected (540.614 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21.\n",
    "\n",
    "0: jdbc:hive2://> DROP DATABASE election CASCADE;\n",
    "OK\n",
    "No rows affected (0.814 seconds)\n",
    "0: jdbc:hive2://> SHOW DATABASES;\n",
    "OK\n",
    "+----------------+--+\n",
    "| database_name  |\n",
    "+----------------+--+\n",
    "| default        |\n",
    "| dualcore       |\n",
    "| tmdb_movies    |\n",
    "| tweets         |\n",
    "+----------------+--+\n",
    "4 rows selected (0.058 seconds)\n",
    "\n",
    "## Delete all filesystems recurisvely in hadoop and local fs\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -rm -r /user/cloudera/elections\n",
    "Deleted /user/cloudera/elections\n",
    "\n",
    "[cloudera@quickstart midterm]$ hadoop fs -rm -r /user/cloudera/counties\n",
    "Deleted /user/cloudera/counties\n",
    "\n",
    "[cloudera@quickstart ~]$ rm -r midterm\n",
    "\n",
    "rm -r /home/cloudera/midterm\n",
    "\n",
    "[cloudera@quickstart ~]$ ls -l\n",
    "total 552\n",
    "drwxrwxr-x 20 cloudera cloudera   4096 Nov 12  2017 anaconda2\n",
    "-rwxrwxr-x  1 cloudera cloudera   5387 Apr  5  2017 cloudera-manager\n",
    "-rwxrwxr-x  1 cloudera cloudera   9964 Apr  5  2017 cm_api.py\n",
    "drwxrwxr-x  3 cloudera cloudera   4096 Jul 31  2017 deprecated_usr_lib\n",
    "-rw-rw-r--  1 cloudera cloudera  20387 Nov 12  2017 derby.log\n",
    "drwxrwxr-x  2 cloudera cloudera   4096 Apr  5  2017 Desktop\n",
    "drwxrwxr-x  4 cloudera cloudera   4096 Apr  5  2017 Documents\n",
    "drwxr-xr-x  2 cloudera cloudera   4096 Jun  3 11:05 Downloads\n",
    "drwxrwsr-x  9 cloudera cloudera   4096 Jul 31  2017 eclipse\n",
    "-rw-rw-r--  1 cloudera cloudera  53655 Apr  5  2017 enterprise-deployment.json\n",
    "-rw-rw-r--  1 cloudera cloudera      0 Jul  8 10:53 exit\n",
    "-rw-rw-r--  1 cloudera cloudera  50515 Apr  5  2017 express-deployment.json\n",
    "-rw-rw-r--  1 cloudera cloudera    684 Aug  2  2017 hive_create_statements.hql\n",
    "-rwxrwxr-x  1 cloudera cloudera   5007 Apr  5  2017 kerberos\n",
    "drwxrwxr-x  2 cloudera cloudera   4096 Apr  5  2017 lib\n",
    "drwxrwxr-x  5 cloudera cloudera   4096 Nov 12  2017 metastore_db\n",
    "drwxr-xr-x  2 cloudera cloudera   4096 Jul 20  2017 Music\n",
    "-rwxrwxr-x  1 cloudera cloudera   4228 Apr  5  2017 parcels\n",
    "drwxr-xr-x  2 cloudera cloudera   4096 Jul 20  2017 Pictures\n",
    "drwxr-xr-x  2 cloudera cloudera   4096 Jul 20  2017 Public\n",
    "drwxrwxr-x  5 cloudera cloudera   4096 Jul 31  2017 scripts\n",
    "-rw-r--r--  1 cloudera cloudera 268140 Jul 31  2017 shakepoems.txt\n",
    "drwxr-xr-x  2 cloudera cloudera   4096 Jul 20  2017 Templates\n",
    "drwxrwxr-x  2 cloudera cloudera   4096 Jul 12 12:58 tmdb_movies\n",
    "drwxrwxr-x  6 cloudera cloudera   4096 Nov 12  2017 training_materials\n",
    "-rw-rw-r--  1 cloudera cloudera  41002 Jul  8 10:54 tweets.java\n",
    "-rw-rw-r--  1 cloudera cloudera     72 Nov 12  2017 Untitled.ipynb\n",
    "drwxr-xr-x  2 cloudera cloudera   4096 Jul 20  2017 Videos\n",
    "drwxrwxr-x 23 cloudera cloudera   4096 Jul 31  2017 workspace\n",
    "drwxrwxr-x  4 cloudera cloudera   4096 Jul 31  2017 workspace.save.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND WE'RE DONE!  THANKS PROFESSOR!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
