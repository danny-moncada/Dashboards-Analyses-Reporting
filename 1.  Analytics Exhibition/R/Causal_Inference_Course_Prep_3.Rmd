---
title: "Final Exam Prep Sessions 9 - 10"
subtitle: "MSBA 6440: Causal Inference via Experimentation"
author: "Danny Moncada (monca016)"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  pdf_document:
    includes:
      in_header: my_header.tex
---

```{r, echo = FALSE}
suppressWarnings(suppressPackageStartupMessages({
library(stargazer)
library(sampleSelection)
library(MASS)
library(AER)
  
require(Synth)
require(ggplot2)
require(ggthemes)
}))
```

# Session 9: Selection Bias, Measurement Error (selection)

![](/Users/danny/Documents/images/WomensWages.png)

![](/Users/danny/Documents/images/WomensWages2.png)

Selection model: $$ Probit (lfp_i) = \gamma * x_i + v_i $$

Outcome model: $$ OLS(Wages_i) = \beta * x_i + \beta_\lambda * IMR_i + \epsilon_i $$

![](/Users/danny/Documents/images/Heckman1.png)
Selection model: if $z^*_i$ is greater than 0, then you will participate, otherwise you will not.

Outcome model: if your $z^*_i$ is greater than 0, then we will observe you, otherwise we ill not.

Assumptions: There are two error terms, and they have a normal distribution/standard deviation.


![](/Users/danny/Documents/images/Heckman2.png)

* We need one variable that affects selection but does not influence the outcome.  This is the instrument variable that affects the selection or influences the outcome only through the selection variable.

* If the coefficient of IMR is not significant, then you can argue that selection bias is not an issue.

```{r}

setwd("~/MSBA 2020 All Files/Spring 2020/MSBA 6440 - Causal Inference via Ecnmtrcs Exprmnt/Week 7 - Instrumental Variables")

MROZ <-read.csv("MROZ.csv")

MROZ$kids <- (MROZ$kidslt6 + MROZ$kidsge6) # Count number of total kids

# Female labor supply (lfp = labour force participation)

## Outcome equations without correcting for selection
# I() means "as-is" -- do calculation in parentheses then use as variable

## Comparison of linear regression and selection model

outcome1 <- lm(wage ~ educ, data = MROZ)
summary(outcome1)
# Education has significant relationship with wages

selection1 <- selection(selection = lfp ~ age + I(age^2) + faminc + kidslt6 + educ, # labor force part. is Y
                        # Family chars MIGHT influence participation in labor force, but have NO affect on
                        # wages EXCEPT for influencing their participation in labor force.
                        outcome = wage ~ educ, 
                        data = MROZ, method = "2step") # 2 step Heckman

summary(selection1)
```
* Predict if someone will participate in labor force; here, education and whether they have kids **does** affect their participation.

* Now our coefficients increase and become more significant

* invMillsRatio is not significant; therefore, selection bias is not statistically significant and thus not a problem.

```{r}
plot(MROZ$wage ~ MROZ$educ)
curve(outcome1$coeff[1] + outcome1$coeff[2]*x, col="black", lwd="2", add=TRUE) # OLS regression
curve(selection1$coeff[7] + selection1$coeff[8]*x, col="orange", lwd="2", add=TRUE) # Heckman model


## A more complete model comparison

outcome2 <- lm(wage ~ exper + I( exper^2 ) + educ + city, data = MROZ)
summary(outcome2)

## Correcting for selection

selection.twostep2 <- selection(selection = lfp ~ age + I(age^2) + faminc + kidslt6 + educ, 
                                outcome = wage ~ exper + I(exper^2) + educ + city, 
                                data = MROZ, method = "2step")
summary(selection.twostep2)
# Still not significant!

selection.mle <- selection(selection = lfp ~ age + I(age^2) + faminc + kids + educ, 
                           outcome = wage ~ exper + I(exper^2) + educ + city, 
                           data = MROZ, method = "mle") # Maximum likelihood estimation
summary(selection.mle)


## Heckman model selection "by hand" ##

seleqn1 <- glm(lfp ~ age + I(age^2) + faminc + kidslt6 + educ, family=binomial(link="probit"), 
               data=MROZ)
summary(seleqn1)

## Calculate inverse Mills ratio by hand ##

MROZ$IMR <- dnorm(seleqn1$linear.predictors)/pnorm(seleqn1$linear.predictors)

## Outcome equation correcting for selection ##

outeqn1 <- lm(wage ~ exper + I(exper^2) + educ + city + IMR, data=MROZ, subset=(lfp==1))
summary(outeqn1)

## compare to selection package -- coefficients right, se's wrong

summary(selection.twostep2)

stargazer(outeqn1,selection.twostep2,type="text",title="Heckman Two-step vs.Heckman by Hand",
          column.labels = c("Heckman By Hand","Heckman Command"))
```

![](/Users/danny/Documents/images/MeasureErrorIV.png)

![](/Users/danny/Documents/images/MeasureErrorDV.png)

![](/Users/danny/Documents/images/MeasureErrorConclusion1.png)

* Reverse regression: use Y as your independent variable, and the slope of the reverse regression/linear regression will BRACKET the true relationship.

* The tru estimate will fall between b/g.

![](/Users/danny/Documents/images/MeasureErrorConclusion2.png)
* There is no systematic bias in our measure; if this is true, then whatever treatment effect we are getting is understated because we are measuring X with error.

```{r}
# Author: Gordon Burtch and Gautam Ray
# Course: MSBA 6440
# Session: Selection and Measurement Error
# Topic: Measurement Error

# X and Y have classical measurement error. The true value are Xt and Yt, but they are meas-
# ured with error.
# X is measured as true X (Xt) plus error (ex)
# Y is measured as true Y (Yt) plus error (ey)
# The mean of Yt, Xt, ey and ex is (10, 7, 0, 0)
# The standard deviation of Yt, Xt, ey, and ex is (4, 8, 3, 6)
# The correlation of Yt and Xt is 0.7; Yt and Xt are uncorrelated with ey and ex; and ey and 
# ex are uncorrelated with each other. 

set.seed(1234)


Yt_Xt_ey_ex <- (mvrnorm(10000, c(10, 7, 0, 0), matrix(c(16, 22.4, 0.0, 0.0, 22.4, 64,0, 0, 0, 
                                                        0, 9,0, 0, 0, 0, 36), ncol = 4)))
Yt <- Yt_Xt_ey_ex[,1]
Xt <- Yt_Xt_ey_ex[,2]
ey <- Yt_Xt_ey_ex[,3]
ex <- Yt_Xt_ey_ex[,4]

Y <- Yt + ey
X <- Xt + ex


# Check everything worked as expected. 

cov(Yt_Xt_ey_ex) # co-variance table
cor(Yt_Xt_ey_ex)
sd(ey)
sd(ex)
sd(Yt)
sd(Xt)

mean (Yt)
mean(Xt)
mean(ey)
mean(ex)



#1. Measurement error in X, underestimates the effect of X on Y. The reliability of mismea-
# surement is the magnitude of the mismeasurement. 

summary(lm(Yt~Xt)) # true regression
# 0.349 is true coefficient

summary(lm(Yt~X)) # faulty regression
# 0.22 is understated coefficient

Reliability <- var(Xt)/var(X)

Reliability

summary(lm(Yt~X))$coefficients[2,1]/ summary(lm(Yt~Xt))$coefficients[2,1]
# The ratio of the coefficient from faulty regression compared to the true regression is
# around the same as the reliability number.


#2. Measurement error in Y, does not influence the coefficient of X, but exaggerartes the 
# standard error of the regression coefficient. 

summary(lm(Yt~Xt))
summary(lm(Y~Xt))
# 0.005 standard error is higher

#3. Given that the measurement error in Y is more innocuous than the measurement error in 
# X, we might run the reverse regression. 
# The coefficient of the regular regression and the inverse of the coefficient of the rev-
# erse regression, bracket the true coefficient. 

regular_reg <- (lm(Yt~X))
b <- summary(regular_reg)$coefficients[2,1]


reverse_reg <- (lm(X~Yt))
reverse_reg_coeff <- summary(reverse_reg)$coefficients[2,1]

g <- 1/(summary(reverse_reg)$coefficients[2,1])

b/g # bracket the true estimate
# 0.31 is R-squared, bracketing result extends to multiple regression

summary((lm(Yt~X)))


#4. If there is a good instrument for Xt, then the true estimate can be recovered.
# Lets say that Z is a good instrument for Xt. Like Xt, Z has a mean of 7 and a sd of 8. Z 
# has a correlation of 0.5 with Xt, and Z is uncorrelated with ey and ex. 
# Z has a correlation of 0.35 with Yt which is the product of 0.7 and 0.5 i.e, the correl-
# ation between Yt and Xt and the correlation between Xt and Z.  


Yt_Xt_ey_ex_Z <- (mvrnorm(10000, c(10, 7, 0, 0, 7), matrix(c(16, 22.4, 0.0, 0.0, 11.2, 22.4, 
                    64,0, 0, 32, 0, 0, 9,0, 0, 0, 0, 0, 36, 0, 11.2, 32, 0, 0, 64), ncol = 5)))

Yt <- Yt_Xt_ey_ex_Z[,1]
Xt <- Yt_Xt_ey_ex_Z[,2]
ey <- Yt_Xt_ey_ex_Z[,3]
ex <- Yt_Xt_ey_ex_Z[,4]
Z <- Yt_Xt_ey_ex_Z[,5]

Y <- Yt + ey
X <- Xt + ex

cov(Yt_Xt_ey_ex_Z)
cor(Xt, Z)
cor(X, Z)
cor(Yt, Z)
cor(Y, Z)

ols_true <- lm((Yt~Xt)) # true regression which should give us correctly exact estimate

ivreg1 <- ivreg(formula=Yt ~ X | Z)

ivreg2 <- ivreg(formula=Y ~ X | Z)

stargazer(ols_true,ivreg1, ivreg2,type="text",title="True vs.Instrumented",
          column.labels = c("True","IV1", "IV2"))
```


### A good instrument can solve measurement problems.

# Session 10: Synthetic Control Method (synth)

![](/Users/danny/Documents/images/SynthControlCali.png)


![](/Users/danny/Documents/images/SynthControlCali2.png)
* Dashed line is 38 states that didn't pass the law, form our control.


![](/Users/danny/Documents/images/SynthCounterFactual.png)
* Build a convex roll-up of states that look like CA in the pre period (a synthetic state).

![](/Users/danny/Documents/images/SynthControlTable.png)
* All of these are predictors of smoking rate

* Synthetic control looks closer to CA than average of 38 states


![](/Users/danny/Documents/images/SynthControlPlot.png)
* Per capita sales

* We can use the dashed line to create a counter factual and determine what would have happened to CA if they didn't pass the law

![](/Users/danny/Documents/images/SynthControlPermMethods.png)
* The CA estimate should be **higher** than for the other states

* If the placebo shows that they are the same, then our analysis is no good - there is no effect nor is it statistically significant


![](/Users/danny/Documents/images/SynthControlPermMethodsPlot.png)
* CA is an outlier from any other state that did not have the treatment

```{r}

setwd("~/MSBA 2020 All Files/Spring 2020/MSBA 6440 - Causal Inference via Ecnmtrcs Exprmnt/Week 10 - Synthetic Control")

#Change the read-in line to wherever your saved version of the fracking data csv file lives
#Note: your panel unit 'names' variable must be a character / string, not a factor, or it won't work.
fracking.data = read.csv("fracking.csv",stringsAsFactors=FALSE)
head(fracking.data)

fracking.data$treated = (fracking.data$state=="California")
ggplot(fracking.data, aes(x=year,y=Y,group=state)) + 
  geom_line(aes(color=treated,linetype=!(treated))) + 
  geom_vline(xintercept=2000,linetype="dashed")

#Let's drop the ID column.
fracking.data = fracking.data[,-c(1)]

# your outcome variable *must* be named Y for Synth to accept it (bad coding practices in 
# here I suspect)
dataprep.out=
   dataprep(foo = fracking.data,
   predictors = c("res.share", "edu", "pop.dense"),
   predictors.op = "mean",
   dependent = "Y",
   unit.variable = "panel.id",
   time.variable = "year",
   
   #Any pre-period X's we want to include using different aggregation function, other than 
   # mean, or different time windows, specific years vs. all years, we enter here.
   special.predictors = list(list("Y", 1999, "mean"),list("Y", 1995, "mean"),list("Y", 1990, "mean")),
   
   #which panel is treated?
   treatment.identifier = 7,
   
   #which panels are we using to construct the synthetic control?
   controls.identifier = c(29, 2, 13, 17, 32, 38),
   
   #what is the pre-treatment time period?
   time.predictors.prior = c(1994:1999),
   
   time.optimize.ssr = c(1994:1999),
   
   #name of panel units
   unit.names.variable = "state",
   
   #time period to generate the plot for.
   time.plot = 1994:2006)

synth.out = synth(dataprep.out)

# Two native plotting functions.
# Path.plot() plots the synthetic against the actual treated unit data. 
path.plot(dataprep.res = dataprep.out, synth.res = synth.out,Xlab="Year",
          Ylab="Income Per Capita",
          Main="Comparison of Synth vs. Actual Per Capita Income in California")
abline(v=2000,lty=2,col="red")

# Gaps.plot() shows the deviation between the synthetic and the actual over time.
gaps.plot(dataprep.res = dataprep.out, synth.res = synth.out,Xlab="Year",
          Ylab="Income Per Capita",Main="ATET Estimate of Fracking Law on Per Capita Income")
abline(v=2000,lty=2,col="red")

controls <- c(29, 2, 13, 17, 32, 38)

# We can pull out the data from the result, to make our own nicer plots in ggplot of course
plot.df = data.frame(dataprep.out$Y0plot%*%synth.out$solution.w) 
years = as.numeric(row.names(plot.df))
plot.df = data.frame(y=fracking.data$Y[fracking.data$state=='California' & 
                              fracking.data$year %in% years]) - data.frame(y=plot.df$w.weight)
plot.df$years <- years
plot.df$state <- "California"
ggplot(plot.df,aes(y=y,x=years)) + 
  geom_line() + 
  geom_hline(yintercept=0,linetype="dashed") + 
  geom_vline(xintercept=2000,linetype="dashed") + xlab("Year") + ylab("Income Per Capita")

# Okay, let's simulate a null distribution
# We'll run synthetic control on each of the untreated units, using the other units as 
# controls (we exclude the treated unit from the control set in each placebo run).
for (i in 1:length(controls)){
  controls_temp <- controls[!controls %in% controls[i]]
  #your outcome variable *must* be named Y for Synth to accept it (bad coding practices in 
  # here I suspect)
  dataprep.out.placebo=
    dataprep(foo = fracking.data,
             predictors = c("res.share", "edu", "pop.dense"),
             predictors.op = "mean",
             dependent = "Y",
             unit.variable = "panel.id",
             time.variable = "year",
             
             #Any pre-period X's we want to include using different aggregation function, 
             # other than mean, or different
             # time windows, specific years vs. all years, we enter here.
             special.predictors = list(list("Y", 1999, "mean"),
                                       list("Y", 1995, "mean"),
                                       list("Y", 1990, "mean")),
             
             # which panel is treated?
             treatment.identifier = controls[i],
             
             # which panels are we using to construct the synthetic control?
             controls.identifier = controls_temp,
             
             # what is the pre-treatment time period?
             time.predictors.prior = c(1994:1999),
             
             time.optimize.ssr = c(1994:1999),
             
             # name of panel units
             unit.names.variable = "state",
             
             # time period to generate the plot for.
             time.plot = 1994:2006)

  synth.out.placebo = synth(dataprep.out.placebo)
  plot.df.temp <- data.frame(dataprep.out.placebo$Y0plot%*%synth.out.placebo$solution.w)
  years = as.numeric(row.names(plot.df.temp))
  plot.df.update <- data.frame(y=fracking.data$Y[fracking.data$panel.id==controls[i] & 
                        fracking.data$year %in% years]) - data.frame(y=plot.df.temp$w.weight)
  plot.df.update$years <- years
  plot.df.update$state <- unique(fracking.data[fracking.data$panel.id==controls[i],]$state)
  plot.df <- rbind(plot.df, plot.df.update)
}

plot.df$treated <- (plot.df$state=="California")

# Let's plot the diffs associated with each control state. 
ggplot(plot.df,aes(y=y,x=years,group=state)) + 
  geom_line(aes(color=treated,linetype=!treated)) + 
  geom_vline(xintercept=2000,linetype="dotted") + 
  geom_hline(yintercept=0)

# Our syntheses for ID, OR and IL are all terrible in the pre period.
# I can remove them here for now, but you'd want to tweak the inputs to try to get a better 
# MSPE for those three.
ggplot(plot.df[plot.df$state!="Idaho" & plot.df$state!="Oregon" & plot.df$state!="Illinois",],
       aes(y=y,x=years,group=state)) + 
  geom_line(aes(color=treated,linetype=!treated)) + 
  geom_vline(xintercept=2000,linetype="dotted") + 
  geom_hline(yintercept=0) + 
  ylab("Gap Between Actual and Synth. Y")

# I can also recover my cumulative alpha (the ATT) for CA and all placebo estimates. 
# by summing over the gaps in the post period.
# If I exclude the 3 poorly synthesized states, CA is the biggest effect in the distribution.
# This is a sparse null distribution, but technically empirical p-value = 0.000. 
post.treats <- plot.df[plot.df$year>=2000,]
alphas <- aggregate(post.treats[-c(2:3)], by=list(post.treats$state),FUN=sum)
View(alphas[alphas$Group.1!="Idaho" & alphas$Group.1!="Oregon" & alphas$Group.1!="Illinois",])
```

![](/Users/danny/Documents/images/SConclusion.png)
* empirical p-values