{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinkley Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.octoparse.com/blog/web-scraping-using-python\n",
    "urls = ['https://www.yelp.com/biz/grand-casino-hinckley',\n",
    "      'https://www.yelp.com/biz/grand-casino-hinckley?start=20',\n",
    "      'https://www.yelp.com/biz/grand-casino-hinckley?start=40']\n",
    "\n",
    "clean_reviews_hinckley = []\n",
    "clean_stars_hinckley = []\n",
    "\n",
    "for url in urls:\n",
    "    review = []\n",
    "    stars = []\n",
    "    ourUrl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(ourUrl,'html.parser')\n",
    "\n",
    "    for i in soup.find_all(\"li\"):\n",
    "        per_review = i.find_all(\"div\",{\"class\":\"review-content\"})\n",
    "        for z in per_review:\n",
    "            text = z.find(\"p\")\n",
    "            review.append(text)\n",
    "            \n",
    "    for i in soup.find_all('img', alt = True):\n",
    "        if \"star\" in str(i):\n",
    "            stars.append(i['alt'].replace(' star rating',''))\n",
    "            \n",
    "    for each in review:\n",
    "        new_each = str(each).replace('<br/>','').replace('\\xa0','')\n",
    "        new_each=new_each[13:-4]\n",
    "        clean_reviews_hinckley.append(new_each)\n",
    "    \n",
    "    clean_stars_hinckley.append(stars[1:-2])\n",
    "    \n",
    "stars_hinckley = [item for sublist in clean_stars_hinckley for item in sublist]\n",
    "\n",
    "\n",
    "stars_hinckley.pop(51)\n",
    "stars_hinckley.pop(38)\n",
    "stars_hinckley.pop(37)\n",
    "stars_hinckley.pop(36)\n",
    "stars_hinckley.pop(13)\n",
    "stars_hinckley.pop(6)\n",
    "stars_hinckley.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_reviews_hinckley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stars_hinckley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onamia Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.octoparse.com/blog/web-scraping-using-python\n",
    "urls = ['https://www.yelp.com/biz/grand-casino-onamia-2',\n",
    "      'https://www.yelp.com/biz/grand-casino-onamia-2?start=20',\n",
    "      'https://www.yelp.com/biz/grand-casino-onamia-2?start=40']\n",
    "\n",
    "clean_reviews_onamia = []\n",
    "clean_stars_onamia = []\n",
    "\n",
    "for url in urls:\n",
    "    review = []\n",
    "    stars = []\n",
    "    ourUrl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(ourUrl,'html.parser')\n",
    "\n",
    "    for i in soup.find_all(\"li\"):\n",
    "        per_review = i.find_all(\"div\",{\"class\":\"review-content\"})\n",
    "        for z in per_review:\n",
    "            text = z.find(\"p\")\n",
    "            review.append(text)\n",
    "            \n",
    "    for i in soup.find_all('img', alt = True):\n",
    "        if \"star\" in str(i):\n",
    "            stars.append(i['alt'].replace(' star rating',''))\n",
    "            \n",
    "    for each in review:\n",
    "        new_each = str(each).replace('<br/>','').replace('\\xa0','')\n",
    "        new_each=new_each[13:-4]\n",
    "        clean_reviews_onamia.append(new_each)\n",
    "    \n",
    "    clean_stars_onamia.append(stars[1:-2])\n",
    "    \n",
    "stars_onamia = [item for sublist in clean_stars_onamia for item in sublist]\n",
    "stars_onamia.pop(7) #this is an updated one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_reviews_onamia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stars_onamia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hinckley = list(zip(clean_reviews_hinckley, binary_stars_hinckley))\n",
    "data_onamia = list(zip(clean_reviews_onamia, binary_stars_onamia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_stars_onamia = []\n",
    "for x in stars_onamia_int:\n",
    "    if x >= 4:\n",
    "        binary_stars_onamia.append(1)\n",
    "    else:\n",
    "        binary_stars_onamia.append(0)\n",
    "        \n",
    "binary_stars_hinckley = []\n",
    "for x in stars_hinckley_int:\n",
    "    if x >= 4:\n",
    "        binary_stars_hinckley.append(1)\n",
    "    else:\n",
    "        binary_stars_hinckley.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hinckley = list(zip(clean_reviews_hinckley, binary_stars_hinckley))\n",
    "data_onamia = list(zip(clean_reviews_onamia, binary_stars_onamia))\n",
    "\n",
    "data = data_hinckley + data_onamia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap the data to get meaningful weights from the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "boot = resample(data, replace=True, n_samples=1000, random_state=1989)\n",
    "boot = np.asarray(boot)\n",
    "\n",
    "features = boot[:,0]\n",
    "labels = boot[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the text features to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3284947fb0b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprocessed_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Remove all the special characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprocessed_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ameyers1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "no matches\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ha and wa show up too often. Let's correct these. Ahead of lemmatizing\n",
    "import re\n",
    "for x in range(0,len(processed_features)):\n",
    "    processed_features[x] = re.sub(r'\\bha\\b', 'has', processed_features[x])\n",
    "    processed_features[x] = re.sub(r'\\bwa\\b', 'was', processed_features[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ameyers1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def get_lemmatized_text(corpus):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) \n",
    "            for review in corpus]\n",
    "\n",
    "processed_features = get_lemmatized_text(processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ameyers1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, min_df=7, max_df=0.8, \n",
    "                             ngram_range=(3,3), analyzer = 'word', \n",
    "                             stop_words=stopwords.words('english'))\n",
    "processed_features_vec = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the features and labels into training and testing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    processed_features_vec, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=500, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(max_iter=500, tol=1e-3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[147   0]\n",
      " [ 30  23]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      1.00      0.91       147\n",
      "          1       1.00      0.43      0.61        53\n",
      "\n",
      "avg / total       0.88      0.85      0.83       200\n",
      "\n",
      "0.7169811320754718\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(roc_auc_score(y_test.astype(int), predictions.astype(int)))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(list(zip(vectorizer.get_feature_names(),\n",
    "                               list(clf.coef_.flatten()))))\n",
    "\n",
    "output = output.sort_values(by=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>think grand casino</td>\n",
       "      <td>-8.430190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>wa good hotel</td>\n",
       "      <td>-6.354808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>smoky nursing home</td>\n",
       "      <td>-5.281178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>black jack dealer</td>\n",
       "      <td>-4.546143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>live music event</td>\n",
       "      <td>-4.546143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>sure lung cancer</td>\n",
       "      <td>-4.331710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>casino grand casino</td>\n",
       "      <td>-4.146251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>able get room</td>\n",
       "      <td>-4.070226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>slot video poker</td>\n",
       "      <td>-4.057301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>penny nickel slot</td>\n",
       "      <td>-4.017469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>go back casino</td>\n",
       "      <td>-3.448243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>lake mille lac</td>\n",
       "      <td>-3.337959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>non smoking room</td>\n",
       "      <td>-3.259448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>grand casino hinckley</td>\n",
       "      <td>-3.030762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>one person get</td>\n",
       "      <td>-2.618057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>non smoking area</td>\n",
       "      <td>-2.153411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>room wa clean</td>\n",
       "      <td>-2.005981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>pillow smelled like</td>\n",
       "      <td>-2.001607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0         1\n",
       "842     think grand casino -8.430190\n",
       "927          wa good hotel -6.354808\n",
       "791     smoky nursing home -5.281178\n",
       "93       black jack dealer -4.546143\n",
       "525       live music event -4.546143\n",
       "832       sure lung cancer -4.331710\n",
       "146    casino grand casino -4.146251\n",
       "25           able get room -4.070226\n",
       "775       slot video poker -4.057301\n",
       "624      penny nickel slot -4.017469\n",
       "404         go back casino -3.448243\n",
       "508         lake mille lac -3.337959\n",
       "566       non smoking room -3.259448\n",
       "432  grand casino hinckley -3.030762\n",
       "591         one person get -2.618057\n",
       "565       non smoking area -2.153411\n",
       "709          room wa clean -2.005981\n",
       "647    pillow smelled like -2.001607"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[output[1]<=-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write output to file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output[1]<=-2].to_csv(r\"C:\\Users\\ameyers1\\Desktop\\bad_trigrams.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
