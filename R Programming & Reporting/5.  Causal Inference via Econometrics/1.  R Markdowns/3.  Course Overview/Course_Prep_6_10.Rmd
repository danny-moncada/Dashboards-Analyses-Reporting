---
title: "Final Exam Prep Sessions 6 - 10"
subtitle: "MSBA 6440: Causal Inference via Experimentation"
author: "Danny Moncada (monca016)"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  pdf_document:
    includes:
      in_header: my_header.tex
---

```{r, echo = FALSE}

## Suppress warnings for loading packages
suppressWarnings(suppressPackageStartupMessages({
library(data.table)
library(readr)
library(dplyr)
library(plm)
library(stargazer)
library(MESS)
library(lmtest)
library(ggplot2)
library(Synth)
library(lubridate)
library(MatchIt)
library(cem)
library(cobalt)
library(Matching)
library(rbounds)
library(tableone)
library(lmtest)
library(AER)
library(lfe)
library(lattice)
library(rdrobust)
library(rdd)
library(sampleSelection)
}))
```

# Session 6: Differences-in-Differences

![Internet's Dirty Secret: Assessing the Impact of Online Intermediaries on HIV Transmiission](/Users/danny/Documents/images/HIVCraig.png)

![](/Users/danny/Documents/images/DiDIntuition.png)

![](/Users/danny/Documents/images/DiDEst1.png)

![](/Users/danny/Documents/images/DiDEst2.png)

![](/Users/danny/Documents/images/DiDSetup.png)

```{r - DiD Regression Data Prep}

setwd("~/MSBA 2020 All Files/Spring 2020/MSBA 6440 - Causal Inference via Ecnmtrcs Exprmnt/Week 6 - Difference in Differences")

#### Load the data ####
MyData = read.csv("TSTV-Obs-Dataset.csv")


#how long is the period of observation?
max(MyData$week)-min(MyData$week)

#How many subjects got TSTV? (Treated)
length(unique(MyData$id[MyData$premium==TRUE]))

#How many subjects did not get TSTV? (Control)
length(unique(MyData$id[MyData$premium==FALSE]))

#In what 'week' does the "treatment" begin?
min(unique(MyData$week[MyData$after==TRUE]))

# As descriptive visualization, let's look at average weekly viewership for both premium and regular viewers
Week_Ave = MyData %>% group_by(week, premium) %>%
  summarise(ave_view = mean(view_time_total_hr)) %>% ungroup()
ggplot(Week_Ave, aes(x = week, y = ave_view, color = factor(premium))) + 
  geom_line() + 
  geom_vline(xintercept = 2227, linetype='dotted') + 
  ylim(0, 6) + xlim(2220,2233) + 
  theme_bw()
```

![](/Users/danny/Documents/images/DiDvsFixed.png)

```{r - DiD Regression Setup}
#### Difference in Differences Regression ####
# Interpret the treatment effect
did_basic = lm(log(view_time_total_hr+1) ~ premium*after, data=MyData)
summary(did_basic)
# a 1% increase in viewing time is 7% increase going from control to premium



# Let's try replacing the treatment dummy with subject fixed effects.
# What happened to the estimate of premium?
did_fe = plm(log(view_time_total_hr+1) ~ premium*after, data = MyData, 
             index=c("id"), effect="individual", model="within")
# Consumers are identified by id; here we want to see each individual subject and their fixed
# effect.

summary(did_fe)
# Similar to output we had before; there is no coefficent for premium.  Because we are at the
# individual user fixed effect, it is perfectly correlated with premium/not premium; this washes
# this cofficient and removes it from the model.

# Further add week fixed effects
did_sfe_tfe = plm(log(view_time_total_hr+1) ~ premium*after, 
                  data = MyData, index=c("id", "week"), 
                  effect="twoway", model="within")
summary(did_sfe_tfe)
# We lose the "after" coefficient because the weeks are perfectly correlated with this.


# Let's try dynamic DiD instead.
did_dyn_sfe_tfe <- lm(log(view_time_total_hr+1) ~ premium + 
                        factor(week) + premium*factor(week), data = MyData)
# for every week we have a sigma, and for every week we have a beta term/coefficient.
summary(did_dyn_sfe_tfe)
# Prior to week 2226, we want the cofficients to be zero (before treatment began).
# The cofficients DO increase after the treatment period, so that's a positive sign.
# Weeks 16:28, this is the interaction effect between premium and week fixed effect.
```

![](/Users/danny/Documents/images/DiDAssump1.png)

![](/Users/danny/Documents/images/DiDAssump2.png)

```{r - AssumptionsPlacebo}
# Let's retrieve the coefficients and standard errors, and create confidence intervals
model = summary(did_dyn_sfe_tfe)
coefs_ses = as.data.frame(model$coefficients[16:28,c("Estimate", "Std. Error")])
colnames(coefs_ses) = c("beta", "se")
coefs_ses = coefs_ses %>%
  mutate(ub90 = beta + 1.96*se, # Upper bound, 2 standard deviations
         lb90 = beta - 1.96*se, # lower bound, 2 SDs
         week = 1:nrow(coefs_ses))

# Let's connect the estimates with a line and include a ribbon for the CIs. 
ggplot(coefs_ses, aes(x = week, y = beta)) + 
  geom_line() + 
  geom_hline(yintercept=0,linetype="dashed") + 
  geom_vline(xintercept=6,linetype="dashed") + 
  geom_ribbon(aes(ymin = lb90, ymax = ub90), alpha = 0.3) + 
  theme_bw()

# If the parallel assumption was held, then the line prior to the treatment effect would 
# be at zero.  But it is not, so this assumption is violated (but the effect is small).

# Time for our placebo test... 
# Let's limit to pre-period data, and shift the treatment date back in time, artificially, 
# and see if we see sig differences pre treatment.
# Again, recall first week when treatment starts

MyDataPre <- MyData[MyData$after==0,]
max(MyDataPre$week)
MyDataPre$after <- MyDataPre$week > 2224
did_log_basic_placebo <- lm(data=MyDataPre,log(view_time_total_hr+1)~premium+after+premium*after)
summary(did_log_basic_placebo)
```

# Session 7: Instrumental Variables (ivreg)

![](/Users/danny/Documents/images/InstrumentalVars1.png)

![](/Users/danny/Documents/images/IVMath.png)
$$ Y = \alpha + \beta_1 \cdot X_1 + \beta_2 + X_2 + u$$

![](/Users/danny/Documents/images/2LS.png)

$$X_2 = \pi_0 + \pi_1 \cdot W + \epsilon $$

$$Y = \alpha + \beta_1 \cdot X_1 + \beta_2 \cdot \hat{X_2} + u$$

![](/Users/danny/Documents/images/IVReqs.png)

```{r}

# Author: Gordon Burtch and Gautam Ray
# Course: MSBA 6440
# Session: Instrumental Variables
# Lecture 7

setwd("~/MSBA 2020 All Files/Spring 2020/MSBA 6440 - Causal Inference via Ecnmtrcs Exprmnt/Week 7 - Instrumental Variables")

MyData1<-read.csv("MROZ.csv")
MyData <- MyData1[MyData1$lfp==1,] #restricts sample to lfp=1

# OLS Model of Wage on Education

ols <- lm(log(wage)~educ+exper+expersq, data=MyData)

# 2SLS Model 'by hand'

educ.ols <- lm(educ~exper+expersq+motheduc, data=MyData)
educHat <- fitted(educ.ols)
wage.2sls <- lm(log(wage)~educHat+exper+expersq, data=MyData)

# IVREG

wage.ivreg <- ivreg(log(wage)~educ+exper+expersq|exper+expersq+motheduc, data=MyData)

stargazer(ols,wage.2sls,wage.ivreg,
          type="text",title="OLS vs 2SLS vs IVREG",
          column.labels = c("OLS","2SLS","IVREG"))


# Setting "diagnostics = TRUE" let's us assess a hausman test, weak IV stats 
# and overidentifying tests of instrument exclusion.
summary(wage.ivreg,diagnostics=TRUE)

wage.ivreg2 <- ivreg(log(wage)~educ+exper+expersq|exper+expersq+motheduc+fatheduc, data=MyData)
summary(wage.ivreg2,diagnostics=TRUE)
```

![](/Users/danny/Documents/images/IVVietnam.png)

![](/Users/danny/Documents/images/IVTerm.png)

![](/Users/danny/Documents/images/IVRelevance.png)

![Cannot be tested empirically](/Users/danny/Documents/images/IVRestriction.png)

![](/Users/danny/Documents/images/IVTestStats.png)

```{r}
# We will first simulate our treatment variable x, endogenous portion of x and its confounder, c
# Here, we refer to the endogenous variation in x as x*
# An easy way to make them confounded is to use the multivariate normal draw function, mvnorm.
xStarAndC <- mvrnorm(1000, c(20, 15), matrix(c(1, 0.5, 0.5, 1), 2, 2))
xStar <- xStarAndC[, 1]
c <- xStarAndC[, 2]

# If you are curious about syntax for mvnorm... ??MASS::mvnorm
# We pass it the number of obs to draw, the means of the two variables, and a covariance matrix.
# In this case, we simulated 1000 draws for two variables, mean 20 and 15, which are 50% correlated. 
cov(xStar,c)

# Now let's simulate our instrument, and make observed X a function of good variation (random stuff)
# and the bad variation, x*.
# By construction, z is a valid instrument for X now, because it is only correlated with the
# good variation, and it has no direct relationship on our eventual y (except through x).
z <- rnorm(1000)
x <- xStar + z

# Now let's simulate the data-generating process to recover y, 
# a function of observed x, its confounder and an error term.
# Here, the true marginal effect of x on y is 2. 
y <- 1 + 2*x + 10*c + rnorm(1000, 0, 0.5)

# Now lets check to make sure we have a problem of confounding.
cor(x, c)
cor(y, c)

# And let's check that the instrument is valid...
cor(x,z)
cor(c,z)

# Okay, let's run the 'true' regression first, controlling for the confounder.
ols_true <- lm(y ~ x + c)
# ... and let's run the endogenous regression, ignoring the confounder.
ols_endog <- lm(y ~ x)
stargazer(ols_true,ols_endog,type="text",
          title="True vs. Endogenous Regression",
          column.labels = c("True","Endogenous"))

# Okay, so let's start working toward IV reg. Let's do the first stage regression and use its predictions in the second stage.
xHat <- lm(x ~ z)$fitted.values
ols_corrected <- lm(y ~ xHat)
stargazer(ols_true,ols_endog,ols_corrected,
          type="text",
          title="True vs. Endogenous vs. Instrumented",
          column.labels = c("True","Endogenous","Manual"))

# Note that the beta is correctly estimated but the standard errors are not if we use this approach.
# The ivreg package will calculate not only this beta, but the right standard errors.
ivreg <- ivreg(formula=y ~ x | z)
stargazer(ols_true,ols_endog,ols_corrected,ivreg,
          type="text",
          title="True vs. Endogenous vs. Manual vs. Instrumented",
          column.labels = c("True","Endogenous","Manual","IV"))

summary(ivreg,diagnostics=TRUE)

# If you have multiple endogenous and instrumental variables in a single regression, you can tell R explicitly which variables to "remove" and what variables to "include" as instruments 'in its place'
# This syntax means "remove" x and include z. 
# Any variables not mentioned after the pipe instrument for themselves (perfect predictors). 
#ivreg <- ivreg(formula=y ~ x | .-x + z)

# Okay, now let's see what happens if we use instruments that are too weak in their association with x. 
x1 <- xStar + 0.9*z
x2 <- xStar + 0.8*z
x3 <- xStar + 0.7*z
x4 <- xStar + 0.6*z
x5 <- xStar + 0.5*z
x6 <- xStar + 0.4*z
x7 <- xStar + 0.3*z
x8 <- xStar + 0.2*z
x9 <- xStar + 0.1*z
x10 <- xStar + 0.01*z

# Now let's simulate the data-generating process to recover y, 
# a function of observed x, its confounder and an error term.
# Here, the true marginal effect of x on y is 2. 
y1 <- 1 + 2*x1 + 10*c + rnorm(1000, 0, 0.5)
y2 <- 1 + 2*x2 + 10*c + rnorm(1000, 0, 0.5)
y3 <- 1 + 2*x3 + 10*c + rnorm(1000, 0, 0.5)
y4 <- 1 + 2*x4 + 10*c + rnorm(1000, 0, 0.5)
y5 <- 1 + 2*x5 + 10*c + rnorm(1000, 0, 0.5)
y6 <- 1 + 2*x6 + 10*c + rnorm(1000, 0, 0.5)
y7 <- 1 + 2*x7 + 10*c + rnorm(1000, 0, 0.5)
y8 <- 1 + 2*x8 + 10*c + rnorm(1000, 0, 0.5)
y9 <- 1 + 2*x9 + 10*c + rnorm(1000, 0, 0.5)
y10 <- 1 + 2*x10 + 10*c + rnorm(1000, 0, 0.5)

ivreg_weak1 <- ivreg(formula=y1 ~ x1 | z)
ivreg_weak2 <- ivreg(formula=y2 ~ x2 | z)
ivreg_weak3 <- ivreg(formula=y3 ~ x3 | z)
ivreg_weak4 <- ivreg(formula=y4 ~ x4 | z)
ivreg_weak5 <- ivreg(formula=y5 ~ x5 | z)
ivreg_weak6 <- ivreg(formula=y6 ~ x6 | z)
ivreg_weak7 <- ivreg(formula=y7 ~ x7 | z)
ivreg_weak8 <- ivreg(formula=y8 ~ x8 | z)
ivreg_weak9 <- ivreg(formula=y9 ~ x9 | z)
ivreg_weak10 <- ivreg(formula=y10 ~ x10 | z)

# The weaker our instrument, the less accurate our final estimate of X's effect becomes.
stargazer(ivreg_weak1,ivreg_weak2,
          ivreg_weak3,ivreg_weak4,
          ivreg_weak5,ivreg_weak6,
          ivreg_weak7,ivreg_weak8,
          ivreg_weak9,ivreg_weak10,
          type="text")

# Let's plot it for interests sake... 
# First we pull out all the beta estimates, and we make a vector of the correlations we used (strength of IVs)
betas <- rep(NA,10)
for (i in 1:10){
  betas[i] <- get(paste0('ivreg_weak',i))$coefficients[2]
}
weakness <- c(.9,.8,.7,.6,.5,.4,.3,.2,.1,.01)

# Now let's plot our recovered betas, against their strength, and include a ref line for true value of 2.0.

p <- xyplot(betas~weakness,
            xlab="Corr(X,Z)",ylab="Beta",ylim=c(1,15),type="b")
update(p, panel=function(...){ 
  panel.xyplot(...) 
  panel.abline(h=2,lty=2,col="red") 
} ) 

# Okay, now let's see what happens as we violate exclusion 
# That is, as we allow z to be correlated to an increasing degree with the confounders in the error term.
# To make this work, we now need to draw all three variables from a joint distribution (good x, z and confounder)
x_C_Z_1 <- mvrnorm(1000, c(20, 15, 10), matrix(c(1,0.5,0.9,0.5,1,.1,.9,.1,1), 3, 3))
x_C_Z_2 <- mvrnorm(1000, c(20, 15, 10), matrix(c(1,0.5,0.9,0.5,1,.2,.9,.2,1), 3, 3))
x_C_Z_3 <- mvrnorm(1000, c(20, 15, 10), matrix(c(1,0.5,0.9,0.5,1,.3,.9,.3,1), 3, 3))
x_C_Z_4 <- mvrnorm(1000, c(20, 15, 10), matrix(c(1,0.5,0.9,0.5,1,.4,.9,.4,1), 3, 3))
x_C_Z_5 <- mvrnorm(1000, c(20, 15, 10), matrix(c(1,0.5,0.9,0.5,1,.5,.9,.5,1), 3, 3))

x11 <- x_C_Z_1[, 1]
x12 <- x_C_Z_2[, 1]
x13 <- x_C_Z_3[, 1]
x14 <- x_C_Z_4[, 1]
x15 <- x_C_Z_5[, 1]
c1 <- x_C_Z_1[, 2]
c2 <- x_C_Z_2[, 2]
c3 <- x_C_Z_3[, 2]
c4 <- x_C_Z_4[, 2]
c5 <- x_C_Z_5[, 2]
z1 <- x_C_Z_1[, 3]
z2 <- x_C_Z_2[, 3]
z3 <- x_C_Z_3[, 3]
z4 <- x_C_Z_4[, 3]
z5 <- x_C_Z_5[, 3]

# What are we doing here? Making versions of z that are increasingly correlated with c.
# Let's store those correlations for our plot later.
exclusion <- rep(NA,5)
for (i in 1:5){
  exclusion[i] <- cor(get(paste0("c",i)),get(paste0("z",i)))
}

# Okay, now let's simulate our Y's
y11 <- 1 + 2*x11 + 10*c1 + rnorm(1000, 0, 0.5)
y12 <- 1 + 2*x12 + 10*c2 + rnorm(1000, 0, 0.5)
y13 <- 1 + 2*x13 + 10*c3 + rnorm(1000, 0, 0.5)
y14 <- 1 + 2*x14 + 10*c4 + rnorm(1000, 0, 0.5)
y15 <- 1 + 2*x15 + 10*c5 + rnorm(1000, 0, 0.5)

ivreg_endog1 <- ivreg(formula=y11 ~ x11 | z1)
ivreg_endog2 <- ivreg(formula=y12 ~ x12 | z2)
ivreg_endog3 <- ivreg(formula=y13 ~ x13 | z3)
ivreg_endog4 <- ivreg(formula=y14 ~ x14 | z4)
ivreg_endog5 <- ivreg(formula=y15 ~ x15 | z5)

stargazer(ivreg_endog1,ivreg_endog2,
          ivreg_endog3,ivreg_endog4,
          ivreg_endog5,type="text")

# Let's plot it again...
# As you can see, as Z becomes less "excluded" we see it yields worse and worse estimates of X's marginal
# Effect on Y. 
betas <- rep(NA,5)
for (i in 1:5){
  betas[i] <- get(paste0('ivreg_endog',i))$coefficients[2]
}
p <- xyplot(betas~exclusion,xlab="Corr(Z,C)",ylab="Beta",ylim=c(1,10),type="b")
update(p, panel=function(...){ 
  panel.xyplot(...) 
  panel.abline(h=2,lty=2,col="red") 
} ) 

# Okay let's do a real example here...
# This dataset is state-level data on cigarette sales, prices
# and taxes. Taxes are used as an instrument for prices here.
data("CigarettesSW")
sales <- lm(log(packs) ~ log(price) + year + state, data=CigarettesSW)
sales_iv <- ivreg(log(packs) ~ log(price) + year + state | .-log(price) + tax, data = CigarettesSW)
stargazer(sales,sales_iv,
          title="OLS vs. IV",
          type="text",
          column.labels = c("OLS","IV"),
          omit=c("state","year"))

# Setting "diagnostics = TRUE" let's us assess a hausman test, weak IV stats and overidentifying tests of instrument exclusion.
summary(sales_iv,diagnostics=TRUE)
```

# Session 8: Regression Discontinuity (rdrobust)

![](/Users/danny/Documents/images/RegDiscont1.png)
* Measure the difference of the likelihood of students finishing college after getting a 79 on the PSAT and getting scholarship vs students who did not.

![Marginal or the "jump" between margin of victory, at 50% threshold](/Users/danny/Documents/images/RegDiscontDems.png)

```{r}

setwd("~/MSBA 2020 All Files/Spring 2020/MSBA 6440 - Causal Inference via Ecnmtrcs Exprmnt/Week 8 - Regression Discontinuity")

# Author: Gordon Burtch and Gautam Ray
# Course: MSBA 6440
# Session: Regression Discontinuity
# Topic: RDD Example
# Lecture 8

# Dataset used by Lee (2008), which is a paper that talks about the "incumbency advantage"
# in US politics. If I hold a congressional seat right now, to what degree does that increase 
# my party's chances of winning re-election?

# The discontinuity design here is based on vote share in the *last* election. Essentially, I 
# am "assigned" to incumbency: if I won the last election, and not if not. Whether I win an election 
# is based on a simple majority threshold (given the two-party system).  Usually it means I passed 50%
# of the vote. So, we're going to use that 50% cutoff in the last election to estimate the effect of 
# "just barely" winning last time, on winning this time (versus losing).

HouseData <-read.csv("house.csv")

# Let's define our treatment variable (0 = equal proportion of vote in last election)
HouseData$treat <- (HouseData$x>0)
# Your vote difference is greater than 0, you won and are the incumbent.

# Let's run the endogenous regression first... says 35% increase in vote share due to winning last 
# election! Of course we know that's wrong... 
ols <- lm(data=HouseData,y~treat)
# y is a function of the vote share in the current election as part of the treated group, those
# that were incumbents.
summary(ols)
## 0.35; 35% greater chance of winning, but not strictly because of incumbency, this is also part
# due to better campagin, better funds.  We need to whittle this down.

# What is this OLS actually estimating? It's a t-test comparing vote outcomes in current election 
# between incumbents and non-incumbents.
ggplot(data=HouseData) + geom_boxplot(aes(y=y,x=treat))

# Let's try RDD now, where we condition on the relationship between y and x, to get at the effect 
# right around the threshold.
# By using "all" the data we are implicitly using a maximum bandwidth (use all of the range of x 
# around c) This says the local treatment effect is 0.11 (11% increase in vote outcome due to the 
# discontinuity).
ols <- lm(data=HouseData,y~treat + x) # Control for how  much did the previous incumbent win by in
# previous election.
summary(ols)
# 11% advantage to being an incumbent.

# Here's a plot of what we are estimating by running this regression.
ggplot(HouseData, aes(y=y,x=x)) + geom_point(aes(col=treat+1),show.legend = FALSE) + geom_vline(xintercept=0,linetype="dashed",color="red") +
  geom_smooth(aes(group=treat,col=as.numeric(treat)),method = "lm",show.legend=FALSE)

# But we don't really believe that incumbents and prior losers are comparable "generally", 
# so we don't trust this bandwidth value, 
# for the same reason we don't trust the OLS more generally... 
# We *might* believe the comparison is fair right around the election win threshold, however. 
# Here, we are zooming in to a 5% differential on either side of the cutoff, h = 0.05.
Pared_House <- HouseData[HouseData$x >= -0.05 & HouseData$x <= 0.05,] # narrow bandwidth to +/- 5%
ggplot(Pared_House, aes(y=y,x=x,col=as.numeric(treat))) + geom_point(show.legend = FALSE) + geom_vline(xintercept=0,linetype="dashed",color="red")
# Looks better... 

# Okay let's run our RDD regression now. Our estimate falls to about 5% with this tighter bandwidth.
house_rdd <- lm(data=Pared_House,y ~ treat + x) 
summary(house_rdd)
# Now incumbent only has 5% advantage compared to challenger.

ggplot(Pared_House, aes(y=y,x=x)) + geom_point(aes(col=treat+1),show.legend = FALSE) + geom_vline(xintercept=0,linetype="dashed",color="red") +
  geom_smooth(aes(group=treat,col=as.numeric(treat)),method = "lm",show.legend=FALSE)
# Jump is 5% difference, which can be seen here.


# Lets try an interaction to see if the slopes are different
house_rdd_int <- lm(data=Pared_House,y ~ treat*x) 
# We want to see if the slop AFTER the treatment effects is the same as the effect before,
# so we add an interaction term to see what happens.
summary(house_rdd_int)
# Slope to the left and right are the same


# Lets try a square term to see if there is any curvilinearity

Pared_House$x_sq <- Pared_House$x*Pared_House$x
house_rdd_sq <- lm(data=Pared_House,y ~ treat + x + x_sq) 
summary(house_rdd_sq)
# Not significant.

# These days, we don't implement it all manually. 
# We use packages that implement algorithms that choose bandwidth, specification and other things 
# for us based on statistics... We probably want to use a weighting function, for example (further 
# away from cutoff, we down-weight you) in tandem with the optimally chosen band-width, etc. 
# rdrobust() chooses everything for you, based on some cross-validation, etc. 
# This says that we are still over-doing it! A more accurate estimate of the effect is actually about 
# just 6%. 
House_Robust_RDD <- rdrobust(HouseData$y,HouseData$x,c=0)
summary(House_Robust_RDD)
# 6% advantage for being an incumbent, as compared to results from earlier in the analysis.
rdplot(HouseData$y,HouseData$x)

# It can't "fix" self-selection, however, so let's again run a density check around the cut-point to 
# evaluate self-selection ("sorting") the number it spits out is the p-value associated with the non-
# parametric test of density differences around the threshold. 
# In this case, the p-value is ~0.19, which is fairly far away from being a problem (no evidence of sorting)
DCdensity(HouseData$x,0,plot=TRUE)
# bins the data set, and centers it around cut off point to demonstrate data is unbalanced.

```
* We cannot reject the null hypothesis that the density is even on both sides of the cutoff which is 0.

* This is the p-value that comes out of the density test; since it is so low, there is likely no self selection happning in the data set.

* If it WAS significant, then there is sorting happening, and the assumption of regression discontuinity is FALSE.

![](/Users/danny/Documents/images/RDD.png)

$$ Y_i = \hat{\beta_0} + \hat{\beta_1}Z_i + \hat{\beta_2}(X_i - X_c) + e_i $$ 

![](/Users/danny/Documents/images/RDDChoice.png)

![](/Users/danny/Documents/images/RDDAssump.png)
* If the treatment effect is known then this is not valid; for example, if you know the cutoff for PSAT is an 80, and as a student you go for the lowest possible score required, then this student should **not** be included at the cutoff because you *could* have done better and chose not to.

![](/Users/danny/Documents/images/RDDViolation2.png)
* Y is not linear here; y as a function of x **must** be continous.


![](/Users/danny/Documents/images/RDDViolation3.png)

![](/Users/danny/Documents/images/RDDViolation4.png)

![](/Users/danny/Documents/images/RDDViolation5.png)

![](/Users/danny/Documents/images/RDDFuzzy.png)

![](/Users/danny/Documents/images/RDDFuzzyEx.png)

# Session 9: Selection Bias, Measurement Error (selection)

![](/Users/danny/Documents/images/WomensWages.png)

![](/Users/danny/Documents/images/WomensWages2.png)

Selection model: $$ Probit (lfp_i) = \gamma * x_i + v_i $$

Outcome model: $$ OLS(Wages_i) = \beta * x_i + \beta_\lambda * IMR_i + \epsilon_i $$

![](/Users/danny/Documents/images/Heckman1.png)
Selection model: if $z^*_i$ is greater than 0, then you will participate, otherwise you will not.

Outcome model: if your $z^*_i$ is greater than 0, then we will observe you, otherwise we ill not.

Assumptions: There are two error terms, and they have a normal distribution/standard deviation.


![](/Users/danny/Documents/images/Heckman2.png)

* We need one variable that affects selection but does not influence the outcome.  This is the instrument variable that affects the selection or influences the outcome only through the selection variable.

* If the coefficient of IMR is not significant, then you can argue that selection bias is not an issue.

```{r}

MROZ <- MyData1

MROZ$kids <- (MROZ$kidslt6 + MROZ$kidsge6) # Count number of total kids

# Female labor supply (lfp = labour force participation)

## Outcome equations without correcting for selection
# I() means "as-is" -- do calculation in parentheses then use as variable

## Comparison of linear regression and selection model

outcome1 <- lm(wage ~ educ, data = MROZ)
summary(outcome1)
# Education has significant relationship with wages

selection1 <- selection(selection = lfp ~ age + I(age^2) + faminc + kidslt6 + educ, # labor force part. is Y
                        # Family chars MIGHT influence participation in labor force, but have NO affect on
                        # wages EXCEPT for influencing their participation in labor force.
                        outcome = wage ~ educ, 
                        data = MROZ, method = "2step") # 2 step Heckman

summary(selection1)
```
* Predict if someone will participate in labor force; here, education and whether they have kids **does** affect their participation.

* Now our coefficients increase and become more significant

* invMillsRatio is not significant; therefore, selection bias is not statistically significant and thus not a problem.

```{r}
plot(MROZ$wage ~ MROZ$educ)
curve(outcome1$coeff[1] + outcome1$coeff[2]*x, col="black", lwd="2", add=TRUE) # OLS regression
curve(selection1$coeff[7] + selection1$coeff[8]*x, col="orange", lwd="2", add=TRUE) # Heckman model


## A more complete model comparison

outcome2 <- lm(wage ~ exper + I( exper^2 ) + educ + city, data = MROZ)
summary(outcome2)

## Correcting for selection

selection.twostep2 <- selection(selection = lfp ~ age + I(age^2) + faminc + kidslt6 + educ, 
                                outcome = wage ~ exper + I(exper^2) + educ + city, 
                                data = MROZ, method = "2step")
summary(selection.twostep2)
# Still not significant!

selection.mle <- selection(selection = lfp ~ age + I(age^2) + faminc + kids + educ, 
                           outcome = wage ~ exper + I(exper^2) + educ + city, 
                           data = MROZ, method = "mle") # Maximum likelihood estimation
summary(selection.mle)


## Heckman model selection "by hand" ##

seleqn1 <- glm(lfp ~ age + I(age^2) + faminc + kidslt6 + educ, family=binomial(link="probit"), 
               data=MROZ)
summary(seleqn1)

## Calculate inverse Mills ratio by hand ##

MROZ$IMR <- dnorm(seleqn1$linear.predictors)/pnorm(seleqn1$linear.predictors)

## Outcome equation correcting for selection ##

outeqn1 <- lm(wage ~ exper + I(exper^2) + educ + city + IMR, data=MROZ, subset=(lfp==1))
summary(outeqn1)

## compare to selection package -- coefficients right, se's wrong

summary(selection.twostep2)

stargazer(outeqn1,selection.twostep2,type="text",title="Heckman Two-step vs.Heckman by Hand",
          column.labels = c("Heckman By Hand","Heckman Command"))
```

![](/Users/danny/Documents/images/MeasureErrorIV.png)

![](/Users/danny/Documents/images/MeasureErrorDV.png)

![](/Users/danny/Documents/images/MeasureErrorConclusion1.png)

* Reverse regression: use Y as your independent variable, and the slope of the reverse regression/linear regression will BRACKET the true relationship.

* The tru estimate will fall between b/g.

![](/Users/danny/Documents/images/MeasureErrorConclusion2.png)
* There is no systematic bias in our measure; if this is true, then whatever treatment effect we are getting is understated because we are measuring X with error.

```{r}
# Author: Gordon Burtch and Gautam Ray
# Course: MSBA 6440
# Session: Selection and Measurement Error
# Topic: Measurement Error

# X and Y have classical measurement error. The true value are Xt and Yt, but they are meas-
# ured with error.
# X is measured as true X (Xt) plus error (ex)
# Y is measured as true Y (Yt) plus error (ey)
# The mean of Yt, Xt, ey and ex is (10, 7, 0, 0)
# The standard deviation of Yt, Xt, ey, and ex is (4, 8, 3, 6)
# The correlation of Yt and Xt is 0.7; Yt and Xt are uncorrelated with ey and ex; and ey and 
# ex are uncorrelated with each other. 

set.seed(1234)


Yt_Xt_ey_ex <- (mvrnorm(10000, c(10, 7, 0, 0), matrix(c(16, 22.4, 0.0, 0.0, 22.4, 64,0, 0, 0, 
                                                        0, 9,0, 0, 0, 0, 36), ncol = 4)))
Yt <- Yt_Xt_ey_ex[,1]
Xt <- Yt_Xt_ey_ex[,2]
ey <- Yt_Xt_ey_ex[,3]
ex <- Yt_Xt_ey_ex[,4]

Y <- Yt + ey
X <- Xt + ex


# Check everything worked as expected. 

cov(Yt_Xt_ey_ex) # co-variance table
cor(Yt_Xt_ey_ex)
sd(ey)
sd(ex)
sd(Yt)
sd(Xt)

mean (Yt)
mean(Xt)
mean(ey)
mean(ex)



#1. Measurement error in X, underestimates the effect of X on Y. The reliability of mismea-
# surement is the magnitude of the mismeasurement. 

summary(lm(Yt~Xt)) # true regression
# 0.349 is true coefficient

summary(lm(Yt~X)) # faulty regression
# 0.22 is understated coefficient

Reliability <- var(Xt)/var(X)

Reliability

summary(lm(Yt~X))$coefficients[2,1]/ summary(lm(Yt~Xt))$coefficients[2,1]
# The ratio of the coefficient from faulty regression compared to the true regression is
# around the same as the reliability number.


#2. Measurement error in Y, does not influence the coefficient of X, but exaggerartes the 
# standard error of the regression coefficient. 

summary(lm(Yt~Xt))
summary(lm(Y~Xt))
# 0.005 standard error is higher

#3. Given that the measurement error in Y is more innocuous than the measurement error in 
# X, we might run the reverse regression. 
# The coefficient of the regular regression and the inverse of the coefficient of the rev-
# erse regression, bracket the true coefficient. 

regular_reg <- (lm(Yt~X))
b <- summary(regular_reg)$coefficients[2,1]


reverse_reg <- (lm(X~Yt))
reverse_reg_coeff <- summary(reverse_reg)$coefficients[2,1]

g <- 1/(summary(reverse_reg)$coefficients[2,1])

b/g # bracket the true estimate
# 0.31 is R-squared, bracketing result extends to multiple regression

summary((lm(Yt~X)))


#4. If there is a good instrument for Xt, then the true estimate can be recovered.
# Lets say that Z is a good instrument for Xt. Like Xt, Z has a mean of 7 and a sd of 8. Z 
# has a correlation of 0.5 with Xt, and Z is uncorrelated with ey and ex. 
# Z has a correlation of 0.35 with Yt which is the product of 0.7 and 0.5 i.e, the correl-
# ation between Yt and Xt and the correlation between Xt and Z.  


Yt_Xt_ey_ex_Z <- (mvrnorm(10000, c(10, 7, 0, 0, 7), matrix(c(16, 22.4, 0.0, 0.0, 11.2, 22.4, 
                    64,0, 0, 32, 0, 0, 9,0, 0, 0, 0, 0, 36, 0, 11.2, 32, 0, 0, 64), ncol = 5)))

Yt <- Yt_Xt_ey_ex_Z[,1]
Xt <- Yt_Xt_ey_ex_Z[,2]
ey <- Yt_Xt_ey_ex_Z[,3]
ex <- Yt_Xt_ey_ex_Z[,4]
Z <- Yt_Xt_ey_ex_Z[,5]

Y <- Yt + ey
X <- Xt + ex

cov(Yt_Xt_ey_ex_Z)
cor(Xt, Z)
cor(X, Z)
cor(Yt, Z)
cor(Y, Z)

ols_true <- lm((Yt~Xt)) # true regression which should give us correctly exact estimate

ivreg1 <- ivreg(formula=Yt ~ X | Z)

ivreg2 <- ivreg(formula=Y ~ X | Z)

stargazer(ols_true,ivreg1, ivreg2,type="text",title="True vs.Instrumented",
          column.labels = c("True","IV1", "IV2"))
```
### A good instrument can solve measurement problems.

# Session 10: Synthetic Control Method (synth)

![](/Users/danny/Documents/images/SynthControlCali.png)


![](/Users/danny/Documents/images/SynthControlCali2.png)

![](/Users/danny/Documents/images/SynthCounterFactual.png)

![](/Users/danny/Documents/images/SynthControlTable.png)



![](/Users/danny/Documents/images/SynthControlPlot.png)


![](/Users/danny/Documents/images/SynthControlPermMethods.png)

![](/Users/danny/Documents/images/SynthControlPermMethodsPlot.png)




