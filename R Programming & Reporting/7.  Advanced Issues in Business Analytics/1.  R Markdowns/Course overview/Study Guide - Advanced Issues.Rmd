---
title: "Study Guide"
subtitle: "MSBA 6430: Advanced Issues in Business Analytics"
author: "Danny Moncada (monca016)"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  pdf_document:
    includes:
      in_header: my_header.tex
---

```{r, echo = FALSE}
suppressWarnings(suppressPackageStartupMessages({
library(TSA)
library(ggplot2)
library(dplyr)
library(forecast)
library(tseries) # ADF Test for Stationarity
library(igraph)
}))
```
# Time Series Analysis

## Lecture 2: Stationarity

A. Mean function: $\mu_t$ = E($Y_t$) for any *t* (any point).

B. Auto-covariance: $\gamma_{t,s}$ = Cov($Y_t$, $Y_s$) for any *t*, *s* (between any two points).

C. Auto-correlation: $\rho_{t,s}$ = Corr($Y_t$, $Y_s$) for any *t*, *s* (between any two points). 
The correlation of a signal with a delayed copy of itself as a function of delay.  Informally, it is the similarity between observations as a function of the time lag between them.

## Stationarity:

#### Why is stationarity so important?
One sample path can infer the stochastic process.  Stationarity implies that the process is at an equilibrium.  It has a well-defined constant mean level and just jumps around for eternity in the same way (deviating approximately at the same scale, and having the same level of correlation with previous observations.)
A process $Y_t$ is stationary if:  
1. Mean function E($Y_t$) = c is constant over time  
2. Auto-covariance function Cov($Y_t$, $Y_{t-k}$) = $\gamma_{t}$

#### Stochastic process vs. sample path

1. *Stochastic process* is a sequence of random vairables {$Y_t$|t = 0, 1, 2,...}.  $Y_1$ is coin 1, $Y_2$ is coin 2.  
2. *Sample path* of a stochastic process is just ONE sample (realization) of that stochastic process (sequence of random variables).  E.g. {H, H, T, H, T, T, ...} result of flipping coins.  
3. One stocahstic process can generate many sample paths (infinitely many).  


#### White noise
1. $Y_t$ = $e_t$
2. Called "white noise" because it looks like white light on the spectrometers.
3. Part of variation beyond human's control / understanding.
4. Yes, it is stationary because the mean function E($Y_t$) = 0, and the auto-covariance function Cov($Y_t$, $Y_s$) = 0 (every $Y_t$ is independent of every $Y_s$).
```{r, fig.width = 15, fig.height = 5}
set.seed(12345)
e <- rnorm(100)
Y <- ts(e)

plot(Y, type = 'o')
```

#### Moving Average (never forget e_t!)
1. $Y_t$ = $e_t$ + $e_{t-1}$
2. Only formulates first order moving dependency but you cannot predict more than _n_ time points ahead.
3. You can do short term forecasting (and why we need AR models for long term).
4. Yes, it is stationary because mean is constant zero and auto correlation is indepedent of _t_ (terms are uncorrelated).
```{r, fig.width = 15, fig.height = 5}
set.seed(12345)
e <- rnorm(100)
Y <- ts((e + zlag(e)))
par(mar=c(2,2,0.2,0.2)); plot(Y, type = 'o')
```

#### Random Walk
1. $Y_t$ = $e_1$ + $e_2$ + $e_3$... + $e_k$
2. No, it is **not** stationary; you accumulate new knowledge with no attrition (you never forget).  The mean is always the same during random walk (**0**) but the variance increases expontentially, because all past values are being included.
```{r, fig.width = 15, fig.height = 5}
set.seed(12345)
e <- rnorm(100)
Y <- ts(cumsum(e))
par(mar=c(2,2,0.2,0.2)); plot(Y, type = 'o')
```
![](/Users/danny/Documents/images/Random_Walk_Var.png)


## Lecture 4: ARMA

## MA - Moving Average

1. $Y_t$ is a function of only white noise $e_{t-1}$, $e_{t-2}$... 

#### MA(1)

1. $Y_t$ = $e_t$ - $\theta_1e_{t-1}$ (some constant)
2. First-Order MAP
3. Stationarity: Yes.

![Mean, Variance, lag-k auto-covariance for MA(1) process](/Users/danny/Documents/images/MA1_Calcs.png)

#### MA(2)

1. $Y_t$ = $e_t$ - $\theta_1e_{t-1}$ - $\theta_2e_{t-2}$ (some constants)
2. Second-Order MAP
3. Stationarity: Yes.

![Mean, Variance, lag-k auto-covariance for MA(2) process](/Users/danny/Documents/images/MA2_Calcs.png)


#### MA(q)

1. $Y_t$ = $e_t$ - $\theta_1e_{t-1}$ - $\theta_2e_{t-2}$-...-$\theta_qe_{t-q}$ (some constants)
2. *q*th-Order MAP
3. Stationarity: Yes.

```{r, fig.width = 15, fig.height = 5}
set.seed(12345)
e <- rnorm(100)
Y <- ts(e + 0.5*zlag(e) + 0.5*zlag(e, 2) + 0.5*zlag(e, 3) + 0.5*zlag(e, 4))
ggAcf(Y)
```

## AR - Autoregressive model

1. $Y_t$ is a function of its past $Y_{t-1}$, $Y_{t-2}$...

#### AR(1)

1. $Y_t$ = $\phi_1Y_{t-1}$ + $e_t$ (where $\phi_1$ is a constant who absolute value is less than 1; if it is equal to 1 then it is a random walk).
2. First-Order ARP
3. Backshift operator: (1 - $\phi$B)$Y_t$ = $e_t$

![Mean, Variance, lag-k auto-covariance for AR(1) process](/Users/danny/Documents/images/AR1_Calcs.png)

#### AR(2)

1. $Y_t$ = $\phi_1Y_{t-1}$ + $\phi_2Y_{t-2}$ + $e_t$ (where $\phi_1$, $\phi_2$ are constants).
2. Second-Order ARP
3. Backshift operator:

#### AR(p)

1. $Y_t$ = $\phi_1Y_{t-1}$ + $\phi_2Y_{t-2}$ + $\phi_pY_{t-p}$ + $e_t$
2. *p*th-Order ARP
3. Backshift operator:

#### What is the general definition of ARMA(p, q) (never forget e_t!)?

ARMA(p,q) is Auto-Regressive Moving Average Process of orders *p* and *q*.

General formula:
$$Y_t = \phi_1Y_{t-1} + \phi_2Y_{t-2} + ... + \phi_pY_{t-p} + e_t - \theta_1e_{t-1} - \theta_2e_{t-2}-...-\theta_qe_{t-q}$$

Backshift operator:
$$Y_t - \phi_1Y_{t-1} - \phi_2Y_{t-2} - ... - \phi_pY_{t-p} = e_t - \theta_1e_{t-1} - \theta_2e_{t-2}-...-\theta_qe_{t-q}$$
This is why we use "minus" sign in definition of MA.
$\Phi(B)Y_t$ = $\Theta(B)e_t$

![](/Users/danny/Documents/images/ARMA_Backshift.png)


#### Is ARMA(p,q) stationary?
The MA part is ALWAYS stationary.  Thus, the stationarity of ARMA(p,q) only relies on AR part, so if AR(p) is stationary, whole process is stationary.

#### How do you check stationarity of AR(p) or ARMA(p,q) if the model is known?  Also, how to draw a conclusion if the roots of the characteristic polynomial is given?

1. We have to check the characteristic polynomial. 

2. AR(p) is stationary if $\Phi$ = 0 has all roots >= 1 or <= -1.  We can check this using *polyroot()*.

3.
$Y_t$ = 0.8$Y_{t-1}$ + 0.6$Y_{t-2}$ + $e_t$

$\Phi$(x) = 1 - 0.8x - 0.6x^2
```{r}
polyroot(c(1, -0.8, -0.6))
```
Not stationary, because 0.7 is not greater than 1.

$Y_t$ = 2.2$Y_{t-1}$ - 1.57$Y_{t-2}$ + 0.36$Y_{t-3}$ + $e_t$
```{r}
polyroot(c(1, -2.2, 1.57, -0.36))
```
Stationary, all roots >1.

#### Why do we need both AR and MA models? (Which one has short/long memory?)

MA models short-term dependence.  AR models long-term dependence.  ARMA is as *fundamental as linear regression*.  It is the **best "linear approximation" to any arbitrary stationary process**.

## Lecture 5: SARIMA

## Differencing

#### Why do we need to take differences?
Stationarity is what grants us magic power to discover the full process from *one* sample path, and without this, we cannot say anything about the full process.  Real world data is rarely stationary, so we can difference to coerce the process to a stationary one.

#### Demonstrate that differencing makes these processes stationary.

1. Random walk: $Y_t$ = $Y_{t-1}$ + $e_t$
If we define a new process, $Z_t$ = $Y_t$ - $Y_{t-1}$ = $e_t$, this $Z_t$ is white-noise and stationary (taking a first difference).

2. Stationary time series with linear trend: $Y_t$ = $t$ + $e_t$ is not stationary.

$$Zt = \Delta Y_t = t + e_t - [(t-1) + e_{t-1}] = 1 + e_t - e_{t-1}$$
IS STATIONARY.  

$Y_t$ = $t^2$ + $e_t$ is not stationary.

$$Zt = \Delta Y_t [t^2 + e_t] - [(t-1)^2 + e_{t-1}] = 2t - 1 + e_t - e_{t-1}$$

3.  Simple ARIMA (up to ARIMA (1,1,2))

![](/Users/danny/Documents/images/SimpleARIMA_Backshift.png)


#### Write the d-th order differencing with backshift operator.

In general, $Y_t$ = $p_d$(t) + $e_t$ is stationary after *d*th-order polynomial, e.g. $p_4$($t$) = 2.7$t^{4}$ + 3.6$t^{3}$ + 1.2$t^{2}$ + $t$ + 0.5

#### Write simple ARIMA (up to ARIMA(1,1,2)) with backshift operator.

![ARIMA(1,1,1) using backshift operator](/Users/danny/Documents/images/ARIMA112_Backshift.png)

$$(1-\phi_1B)(1-B)Y_t = (1-\theta_1B)e_t - (1-\theta_2B)^2e_{t-2}$$


#### Read order and estimated coefficients of ARIMA(p,1,q) from R, and recover underlying time series model.
NEVER FORGET e_T on any model - free points!

$$Y_t - Y_{t-1} = 0.83(Y_{t-1}-Y_{t-2}) + e_t + 0.71 \cdot e_{t-1}-0.22 \cdot e_{t-2})$$

## Seasonality

#### What does each order of SARIMA(p,d,q)(P,D,Q)[S] mean? (p is degree of non-seaonsal AR part, Q is the degree of the seasonal MA part)

* Non-seasonal
  + *p*: degree of non-seasonal AR part
  + *d*: degree of differencing
  + *q*: degree of non-seasonal MA part

* Seasonal
  + *P*: degree of seasonal AR part
  + *D*: degree of seasonal differencing
  + *Q*: degree of seasonal MA part

* S: period



![Multiplicative Seasonal ARIMA](/Users/danny/Documents/images/Multi_SARIMA.png)

#### Write Seasonal AR(p) and Seasonal MA(q)

1. Seasonal AR(p) or SAR(1)[12]
$$Y_t = \phi Y_{t-12} + \phi Y_{t-24} + ... + \phi Y_{t-p} + e_t $$

2. Seasonal MA(q) or SMA(1)[12]

$$Y_t = e_t - \theta_1  e_{t-12} - \theta_2 e_{t-24} - ... - \theta_q e_{t-q}$$

#### Read the order and estimated coefficients of SARIMA (up to SARIMA(1,1,1)(1,1,1)[S] from R)

```{r}
# This example is a *multiplicative* seasonal ARIMA!!
set.seed(1234)
data(co2)
arima_est <- auto.arima(co2)
arima_est
```

Full functional form (rounded to two digits):
$$(1-B^{12})(1-0.83B)Y_t = 0.15 + (1-0.46B)(1-0.85B^{12})e_t$$
where $Var[e_t]$ = 0.5288.

## Lecture 6: Model Selection

Model Selection Tools:

![Model Selection](/Users/danny/Documents/images/All_Tools.png)

Unit Roots:

![Unit Roots](/Users/danny/Documents/images/Unit_Roots.png)


#### What is the test to identify random walk against stationary time series?
The ADF (Augmented Dickey-Fuller) test.

![ADF Explained](/Users/danny/Documents/images/ADF_Intuition.png)

#### How do you read R results?
1. Null hypothesis: not stationary
2. Alternative hypothesis: stationary
3. p-value < 0.05, reject the null hypothesis and conclude process is stationary.

```{r}
#      Augmented Dickey-Fuller Test

# data: y
# Dickey-Fuller = -2.4591, Lag order = 21, p-value = 0.3839
# alternative hypothesis: stationary

# The p-value is not low enough to reject the null hypothesis, so we cannot conclude that 
# the process is stationary.  We would have to conclude that this process is a random walk.
```

```{r}
set.seed(12345)
rand_walk <- cumsum(rnorm(100))
adf.test(rand_walk)
```
p value is high so can’t reject the null hypothesis $H_0$ in favor of $H_1$
  + H0: unit root
  + H1: stationary

#### Determine orders using ACF and PACF plots

AR(p):

```{r, fig.width = 15, fig.height = 5}
set.seed(12345)
Y = arima.sim(model=list(ar=0.9), sd=1, n = 100)
Y2 = arima.sim(model=list(ar=0.9,0.5), sd=1, n=100)

ggAcf(Y) + labs(title = "ACF for AR(1)")
ggPacf(Y) + labs(title = "PACF for AR(1)")

ggAcf(Y2) + labs(title = "ACF for AR(2)")
ggPacf(Y2) + labs(title = "PACF for AR(2)")
```

MA(q):

```{r, fig.width = 15, fig.height = 5}
set.seed(12345)
e <- rnorm(100)
Y <- ts((e - 0.5*zlag(e)))
Y2 <- ts((e - 0.5*zlag(e) - 0.9*zlag(e, 2)))

ggAcf(Y) + labs(title = "ACF for MA(1)")
ggPacf(Y) + labs(title = "PACF for MA(1)")

ggAcf(Y2) + labs(title = "ACF for MA(2)")
ggPacf(Y2) + labs(title = "PACF for MA(2)")
```

#### How do you read ACF/PACF plots? What does the value (height of black bar) at each time lag mean?

#### Can we determine the order of ARMA(p,q) by ACF and PACF?

![ACF and PACF for ARMA Models](/Users/danny/Documents/images/ARMA_ACF_PACF.png)

```{r, fig.width = 15, fig.height = 5}
set.seed(12345)
Y = arima.sim(model=list(ar=0.9, ma=-0.5), sd=1, n = 100)
ggAcf(Y) + labs(title = "ACF for ARMA(1,1)")
```

```{r, fig.width = 15, fig.height = 5}
set.seed(88)
Y = arima.sim(model=list(order=c(2,0,2),ar=c(0.5,0.45), ma=c(0.8,0.5)), sd=1, n = 100)
ggAcf(Y) + labs(title = "ACF for ARMA(2,2)")
ggPacf(Y) + labs(title = "PACF for ARMA(2,2)")
```


#### How to read EACF result?

![EACF](/Users/danny/Documents/images/EACF_Slide.png)

```{r, fig.width = 15, fig.height = 5}
set.seed(88)
Y = arima.sim(model=list(order=c(2,0,3),ar=c(0.3,0.2), ma=c(0.2,0.3, 0.2)), sd=1, n = 100)
ggAcf(Y) + labs(title = "ACF for ARMA(2,3)")
ggPacf(Y) + labs(title = "PACF for ARMA(2,3)")
eacf(Y)
```
Pick the first 'o' in the upper right corner.

#### Akaike Information Criterion (AiC)

$$ AIC = -2log(max \space likelihood) + 2k$$


#### Bayesian Information Criterion (BiC)

$$ BIC = -2log(max \space likelihood) + klog(n)$$
  + BiC has the bigger penalty in general.
  + BIC penalizes the extra model parameters more heavily than AIC. 
  + as *n* approaches infinity, BIC would tend to converge to a specific model in the set of models; AIC wouldn’t – it may pick more and more complex models
  
#### How to compare different models by using AiC or BiC?

The model that has the *lowest* is generally the best model.

```{r}
#AR (2) : Likelihood = 100, k = 2
#ARMA (2,2): Likelihood = 100, k = 4

#AIC ( AR(2) ) = -2 log100 + 2 * 2 = C + 4
#AIC ( AR(4) ) = -2 * log100 + 2 * 4 = C + 8 

#AIC ( AR(2) ) = -2 * 99 + 2 * 2 = -195
#AIC ( ARMA(2, 2) ) = -2 * 100 + 2 * 4 = -192 
```

![Box-Jenkings Approach to Time Series](/Users/danny/Documents/images/BoxJenkingsGuide.png)

## Lecture 7: Forecasting

#### Simple Exponential Smoothing
$$\hat Y_{t+1} = \alpha Y_t + \alpha (1-a)Y_{t-1} + \alpha(1-a)^2Y_{t-2} + ... $$
where $$ 0 \le x \le 1 $$

#### SES Smoothing:
$$(1-B)Y_t = (1-B+ \alpha B)e_t = (1-\theta B)e_t $$

which is clearly describing an ARIMA(0,1,1).

![Other Smoothing Methods](/Users/danny/Documents/images/SmoothingMethods.png)


#### Given Y_1, Y2, ... Yt, how to forecast Y_{t+1}, Y_{t+2}, ..., Y_{t+h} with AR(p)? p<=2, h<=3

General form ($e_t$ = 0, non-zero mean):
$$\hat{Y}_{101} - intercept =  coeff \cdot (Y_{100} - intercept) + coeff \cdot (Y_{99} - intercept)$$

#### Given Y_1, Y2, ... Yt, how to forecast Y_{t+1}, Y_{t+2}, ..., Y_{t+h} with MA(q)? q<=2, h<=3

General form ($e_t$ = 0, non-zero mean):
$$\hat{Y}_{101} = coeff \cdot \hat{e}_{100}$$
$$\hat{Y}_{101} - intercept = coeff \cdot (Y_{100} - \hat{Y}_{100})$$

#### Given Y_1, Y2, ... Yt, how to forecast Y_{t+1}, Y_{t+2}, ..., Y_{t+h} with ARMA(p,q)? p<=2, q<=2, h<=3.

General form ($e_t$ = 0), non-zero mean:
$$\hat{Y}_{101} - intercept = coeff \cdot (Y_{100} - intercept) + coeff \cdot (Y_{99} - intercept) - coeff \cdot e_{t-1}$$

$$\hat{Y}_{102} - intercept = coeff \cdot (\hat{Y}_{101} - intercept) + coeff \cdot (Y_{100} - intercept)$$

![AR(1) Forecasting](/Users/danny/Documents/images/AR1Forecast.png)

![AR(1) with Non Zero Mean Forecasting](/Users/danny/Documents/images/AR1NonZeroForecast.png)

![AR(2) Forecasting](/Users/danny/Documents/images/AR2Forecast.png)

![MA(1) Forecasting](/Users/danny/Documents/images/MA1Forecast.png)
We substitue $e_{t+1}$ with its means E($e_{t+1}$) = 0

![MA(1) Forecasting, Yt+2](/Users/danny/Documents/images/MA1Forecast2.png)
Because MA(1) is a short order model and only carries one term forward the covariance of Y100 and Y102 is 0, which leads to Y102 being 0.

![MA(1) Forecasting, Yt+2](/Users/danny/Documents/images/MA1ForecastNonZero.png)
Subtract the means, just like the AR model, so it comes down to 0 by subtracting mean.

![MA(q) Forecasting](/Users/danny/Documents/images/MAQForecast.png)

![ARMA(p,q) Forecasting](/Users/danny/Documents/images/ARMAForecast.png)


#### What is a prediction interval? How do you interpret the result?

Users can have an expectation (of lower bound and upper bound) ahead of time.  In a business setting, you want to be able to plan for best and worst case scenarios - so there is some flexibility in your planning and you can pivot accordingly.

Commonly used prediction intervals  

* 95% prediction interval

$$\hat Y_t \pm 1.96 \hat\sigma_t $$

where $\pm$ 1.96 is the 2.5% and 97.5% quantile of N(0,1).

* 80% prediction interval

$$\hat Y_t \pm 1.28 \hat\sigma_t $$
where $\pm$ 1.28 is the 10% and 90% quantile of N(0,1)

$\hat\sigma_t$ is the estimated standard deviation at $t$.


![Quantile](/Users/danny/Documents/images/Quantile.png)

```{r, fig.width = 15, fig.height = 5}
set.seed(88)
Z = arima.sim(model=list(order=c(1,0,1),ar=0.8, ma=0.6), sd=1, n = 100)
model3 = auto.arima(Z,d=0,seasonal=F)
autoplot(forecast(model3, h=20))
```
Light blue is 80% c.i., dark blue is 90%.

#### What is an ARMAX model?

An ARMAX model is ARMA (or ARIMA) with covariates.

$$Y_t = \beta X_t + ( \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + ... + \phi_p Y_{t-p}) + e_t - (\theta_1e_{t-1} + \theta_2e_{t-2} + ... + \theta_q e_{t-q}) $$

Where $X_t$ is covariate at time *t* and $\beta$ is its coefficent.
$X_t$ (S&P 500) could be another time series like $Y_t$ (Dow Jones).
$X_t$ can also be "static" variable, like gender, holiday/season indicator, location.


#### Is B (beta) the effect on Y_t as it is in linear regression?

No, the actual actual effect is $\beta$/$\Theta$(B) rather than $\beta$.

![Beta Effect](/Users/danny/Documents/images/BetaEffect.png)
Solution:
Fit regression ($Y_t$ ~ $\beta X_t$) first, so that $\beta$ has regular interpretation.  

Fit residual ($Y_t$ ~ $\hat \beta X_t$) with ARIMA models.

## Lecture 8: Trend

#### Definitions:

1. Constant trend:

$$Y_t = \mu + Z_t $$

where $\mu$ does not depend on t, $E(Z_t)$ = 0 and $Z_t$ is stationary. 

![Stationary?](/Users/danny/Documents/images/ConstantTrend.png)

![Estimating mu - traditional approach](/Users/danny/Documents/images/ConstantTrendEstimation.png)
All the data in a time series is highly correlated, so random slection from the data will lead to HIGH bias.

2. Linear trend:
$$Y_t = \beta_0 + \beta_1t + Z_t $$

This is not stationary; $Z_t$ is 0 but the mean is increasing every few steps.

```{r, fig.width = 15, fig.height = 5}
set.seed(12345);
e <- rnorm(100, 0, 50)
t <- 1:100
Y <- ts(25.0 + 3.0*t + e)
plot(Y, type = 'o')
```

3. Seasonal trend:
$$Y_t = \mu + Z_t $$

where $\mu$ = $\mu_{t-k}$;


Seasonal trend in monthly data:
$$\mu_1 = \mu_13 = \mu_25 = ...$$
$$\mu_2 = \mu_14 = \mu_26 = ...$$

This is not stationary.  Seasonality implies consistent differences in the means of sample paths at different times:
$E(Y_{Jan2018})$ $\ne$ $E(Y_{June2018})$

![Seasonal Trend - Stationary](/Users/danny/Documents/images/SeasonalTrendStationary.png)


4. Any combination of three (linear + seasonal trend): hybrid trend.  Split and model separately (which we did in Workday 2).

#### How do you conduct sequential modeling?

![Sequential Model - Fitting](/Users/danny/Documents/images/SequentialModelingFit.png)
Suppose you want to fit this, a linear model and time series model.  In order to do this, even though you don't know the linear part, you can just fit the linear part first, get the estimate/model residual, and then add the two parts together.  If you scale back to the original model, then it will do it after the transformation.

![Sequential Model - Forecasting](/Users/danny/Documents/images/SequentialModelingForecast.png)
![Sequential Model - Summary](/Users/danny/Documents/images/SequentialModelingSummary.png)

#### Does auto-correlation of Y_t's affect the estimation of the trend?
Yes, read through the slides below.

#### What's the difference between a linear trend and a random walk?

![Linear Differences 1](/Users/danny/Documents/images/LinearDiff1.png)

![Linear Differences 2](/Users/danny/Documents/images/LinearDiff2.png)

![Linear Differences 3](/Users/danny/Documents/images/LinearDiff3.png)


#### What's the consequence of fitting a random walk with a linear model?
Fit a trend which does not exist.  Predicted values keep increasing with t, while random walk has mean 0 and can decrease anytime.

#### How to check if a regression is spurious, i.e. a random walk fit with linear trend

1. Regression diagnostics to check residual correlation

![Residual Plot](/Users/danny/Documents/images/ResidualPlot.png)
Clearly they are not independent variables.

2. QQ plot

![QQPlot](/Users/danny/Documents/images/QQPlot.png)
If it perfectly normal, then all of these points should align perfectly on the red line.

3. ADF test

## Lecture 10 & 11: Social Network Analysis

#### Definitions

![Graphs](/Users/danny/Documents/images/Graphs.png)

A. Vertex:

![Vertex](/Users/danny/Documents/images/Vertex.png)

B. Edge: each edge is a pair of vertices ($v_1$, $v_2$)

C. Degree: number of edges a vertex has

![Degree](/Users/danny/Documents/images/Degree.png)

D. Density:

$$  \frac {number \space of \space edges} {number \space of \space total \space possible \space edges} $$
![Density](/Users/danny/Documents/images/Density.png)
The plot is denser. All possible edges are connected. The network is much smaller than the other one.

E. Clique: Cliques are subsets of vertices, all adjacent to each other, which is also called subgraphs.

![Clique](/Users/danny/Documents/images/Clique.png)
Every triangle is a clique, and for maximum clique, no one else can be included in this clique.

F. 
1. Directed network: ($v_1$, $v_2$) $\ne$ ($v_2$, $v_1$)
2. undirected networks: ($v_1$, $v_2$) $=$ ($v_2$, $v_1$)

```{r}
E <- matrix(c(1,2,1,3,1,4,1,5,1,6,1,7,1,8,1,9), ncol = 2, byrow = TRUE)

g1 <- graph_from_edgelist(E, directed=TRUE)
par(mar=c(0,0,0,0));
plot(g1) + title(main = "Directed Graph")

g2 <- graph_from_literal(A--B, A--C, A--D, A--E, A--F)
par(mar=c(0,0,0,0));
plot(g2) + title(main = "Undirected Graph")
```

G. Weighted/unweighted networks

![Weighted Graphs](/Users/danny/Documents/images/WeightedGraphs.png)

$$e = (v_1, v_2, w)$$
Weight usually represents capacity, cost or distance for that link. 

## Community Detection

![Community Detection](/Users/danny/Documents/images/CommunityDetection.png)
Influence other groups from her - how do you find the people that bridge the gap?#

### Basic principles

#### Edge Betweenness
* The number of shortest paths going through an edge; the edge with higher betweenness tends to be the bridge between two communities.
* Starts from 1 community

* How does it split?  Cutting on edges - look up HW write-up

![Edge Betweenness](/Users/danny/Documents/images/EBSplit.png)




#### Walk Trap
* Starts from _n_ communities and merges vertices.  It assumes that everyone is their own community, and brings them together.
* How does it merge?

![Walk Trap](/Users/danny/Documents/images/WT.png)

#### Modularity:
* Definition:

$$Q = (\# \space of \space edges \space within \space community) - (Expected \space \# \space within \space community) $$
This is the stopping criteria, when this is maximized.

![Modularity](/Users/danny/Documents/images/Modularity.png)

#### How to compare two community structures

```{r}

Esub=read.table("C:/Users/danny/Downloads/fb400.txt",header=F)
Esub=as.matrix(Esub)
#Graph illustration
par(mfrow=c(1,1))
g <- graph_from_edgelist(Esub, directed=FALSE)

set.seed(66)
plot(g,vertex.label=NA,
vertex.frame.color=NA, vertex.size=7,
edge.arrow.size=0.5, edge.arrow.width=0.5)

#Edge betweenness
t0=proc.time() #Also count the computational time
EB=edge.betweenness.community(g, directed = FALSE) #modularity=0.42
proc.time()-t0 #8.8s

memberEB=membership(EB)
max(memberEB) #41 clusters

#WalkTrap
t0=proc.time()
WT=walktrap.community(g) #modularity=0.41
proc.time()-t0 #0.008s

memberWT=membership(WT)

max(memberWT) #18 clusters

#Illustrate and compare the community detection results
par(mfrow=c(1,2))
set.seed(66)
plot(g,vertex.label=NA, vertex.color=memberEB,
vertex.frame.color=NA, vertex.size=7,
edge.arrow.size=0.5, edge.arrow.width=0.5)
set.seed(66)

plot(g,vertex.label=NA, vertex.color=memberWT,
vertex.frame.color=NA, vertex.size=7,
edge.arrow.size=0.5, edge.arrow.width=0.5)

```



![Method Comparison](/Users/danny/Documents/images/MethodComparison.png)


## Personalization

![Latent Factor Models](/Users/danny/Documents/images/LatentFactorModels.png)


#### Adjacency matrix

Defintion: An important representation of a graph.  The elements of the matrix represent the edges of the graph.  It is applicable both to directed and weighted graphs.

![Adjacency Matrix](/Users/danny/Documents/images/AdMatrix.png)

```{r}
A <- matrix(c(0,1,0,0,0,0, 
              0,0,1,0,0,0, 
              0,0,0,1,0,0, 
              0,0,0,0,1,0, 
              0,0,0,0,0,1, 
              1,0,0,0,0,0),
              ncol = 6, byrow = TRUE)

g <- graph_from_adjacency_matrix(A, weighted = TRUE)
par(mar=c(0,0,0,0)); plot(g, edge.label = E(g)$weight)
```

```{r}
A <- matrix(c(0,2.1,0,0,0,0, 
              0,0,3.4,0,0,0, 
              0,0,0,4.8,0,0, 
              0,0,0,0,7.5,0, 
              0,0,0,0,0,1.3, 
              9.1,0,0,0,0,0),
        ncol = 6, byrow = TRUE)

g <- graph_from_adjacency_matrix(A, weighted = TRUE)
par(mar=c(0,0,0,0)); plot(g, edge.label = E(g)$weight)
```
#### Latent factors (send and receiver attributes)
P: $p_i$ sender-specific latent factor
Q: $q_j$ receiver-specific latent factor

![Missing Data](/Users/danny/Documents/images/MissingData.png)

![Latent Factors 1](/Users/danny/Documents/images/LatentFactors1.png)

![Latent Factors 2](/Users/danny/Documents/images/LatentFactors2.png)


##### How to predict weights (probability) of a link based on P and Q

* Matrix multiplication

Suppose node 2's sender-specific latent factor is p2 = (0, 0.4, 0.6) and node 1's receiver-specific latent factor is q1 = (0.7, 0.1, 0).  How likely will node 2 send a signal invitation to node 1?  

* Likelihood equation:
  + (0 x 0.7) + (0.4 x 0.1) + (0.6 x 0) = 0.04
  + 0.04 is your probability.
  
![Probability 1](/Users/danny/Documents/images/Probability1.png)

![Probability 2](/Users/danny/Documents/images/Probability2.png)

![Probability 3](/Users/danny/Documents/images/Probability3.png)

If you made it all the way here, then you know everything there is to know about time series data, estimation, forecasting, and social network analysis.  And you should be extremely proud.